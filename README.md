# =====================================
# üß† Section 1: Global Config & MT5 Init
# =====================================
import os
import sys
import time
import ctypes
import logging
import random
import numpy as np
import pandas as pd
import MetaTrader5 as mt5
import tensorflow as tf
import talib
from typing import Dict
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit
try:
    from tensorflow.keras.callbacks import (
        ModelCheckpoint,
        CSVLogger,
        EarlyStopping,
        ReduceLROnPlateau,
    )
except ModuleNotFoundError:
    from keras.callbacks import (
        ModelCheckpoint,
        CSVLogger,
        EarlyStopping,
        ReduceLROnPlateau,
    )
try:
    from tensorflow.keras import mixed_precision
    from tensorflow.keras.mixed_precision import LossScaleOptimizer, set_global_policy
    from tensorflow.keras.optimizers import Adam
except ModuleNotFoundError:
    from keras import mixed_precision
    from keras.mixed_precision import LossScaleOptimizer
    from keras.optimizers import Adam
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix

# ‚îÄ‚îÄ‚îÄ ‡πÄ‡∏õ‡∏¥‡∏î Memory Growth ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏à‡∏≠‡∏á VRAM ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# ‚îÄ‚îÄ‚îÄ ‡∏ï‡∏±‡πâ‡∏á Mixed-Precision ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ FP16 ‡∏ö‡∏ô Tensor Cores ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
mixed_precision.set_global_policy('mixed_float16')
base_opt = Adam(learning_rate=1e-4)
opt = LossScaleOptimizer(base_opt)

# -----------------------
# Ensure necessary folders
# -----------------------
os.makedirs("logs", exist_ok=True)
os.makedirs("models/ea22", exist_ok=True)
os.makedirs("models/ea27", exist_ok=True)

# ‚öôÔ∏è Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s:%(levelname)s:%(message)s'
)
logger = logging.getLogger(__name__) 

# üîê Seed for reproducibility
np.random.seed(42)
random.seed(42)
tf.random.set_seed(42)

# üìå Global Parameters
LOOK_BACK       = 30
MAX_HOLD_PERIOD = 18
RISK_PERCENT    = 0.01
SYMBOL          = "EURUSDm"
MIN_TRADES      = 50

# ‚úÖ Dynamic SL/TP Calculation (‡πÅ‡∏ó‡∏ô SL_FACTOR / TP_FACTOR ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏á‡∏ó‡∏µ‡πà)
def calculate_sl_tp_dynamic(entry_price: float, atr: float, position_type: str, rr_ratio: float = 2.0) -> tuple:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Stop Loss ‡πÅ‡∏•‡∏∞ Take Profit ‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏° ATR ‡πÅ‡∏•‡∏∞ Risk-Reward Ratio
    """
    sl_range = np.clip(atr * 3, 0.0005, 0.005)  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô SL ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô
    tp_range = sl_range * rr_ratio

    if position_type.lower() == 'long':
        sl = entry_price - sl_range
        tp = entry_price + tp_range
    elif position_type.lower() == 'short':
        sl = entry_price + sl_range
        tp = entry_price - tp_range
    else:
        raise ValueError("position_type must be 'long' or 'short'")

    return sl, tp

# ‚úÖ Optional: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏ä‡πâ S/R Levels

def calculate_sl_tp_from_sr(df: pd.DataFrame, idx: int, position_type: str) -> tuple:
    candle = df.iloc[idx]
    close = candle['close']
    atr = candle['ATR']

    if position_type == 'long':
        sl = candle.get('support', close - atr)
        tp = candle.get('resistance', close + atr * 2)
    elif position_type == 'short':
        sl = candle.get('resistance', close + atr)
        tp = candle.get('support', close - atr * 2)
    else:
        raise ValueError("Invalid position_type")

    if abs(tp - close) < atr:
        tp = close + atr if position_type == 'long' else close - atr
    if abs(close - sl) < atr:
        sl = close - atr if position_type == 'long' else close + atr

    return sl, tp

def add_confirm_entry_feature(df: pd.DataFrame) -> pd.DataFrame:
    """
    ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå confirm_entry: 1 ‡∏´‡∏≤‡∏Å‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô, 0 ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
    ‡πÉ‡∏ä‡πâ logic ‡πÅ‡∏ó‡πà‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ô + RSI + S/R ‡πÅ‡∏ö‡∏ö simplified (vectorized)
    """
    prev_open = df['open'].shift(1)
    prev_close = df['close'].shift(1)
    cond_long = (
        (df['close'] > df['open']) &
        (df['RSI'] < 70) &
        (df['close'] > prev_open) &
        (df['open'] < prev_close) &
        ((df['resistance'] - df['close']) / df['close'] >= 0.01)
    )
    cond_short = (
        (df['close'] < df['open']) &
        (df['RSI'] > 30) &
        (df['open'] > prev_close) &
        (df['close'] < prev_open) &
        ((df['close'] - df['support']) / df['close'] >= 0.01)
    )
    df['confirm_entry'] = 0
    df.loc[cond_long | cond_short, 'confirm_entry'] = 1
    return df

# =============================
# üß© DLL Load (if needed)
# =============================
def load_dll(path: str):
    try:
        cd = ctypes.CDLL(os.path.abspath(path))
        cd.add.argtypes = (ctypes.c_int, ctypes.c_int)
        cd.add.restype = ctypes.c_int
        logger.info("DLL loaded test: %d", cd.add(10, 20))
        return cd
    except Exception as e:
        logger.error("DLL load error: %s", e)
        sys.exit(1)

DLL_PATH = r"C:\Users\ACE\OneDrive - Khon Kaen University\Desktop\MyLibrary\x64\Debug\MyLibrary.dll"
DLL = load_dll(DLL_PATH)

def get_account_balance() -> float:
    info = mt5.account_info()
    return info.balance if info else 100000.0

# =====================================
# üìä Section 2: Data Loader & Indicators
# =====================================
import talib

# üîÅ Load CSV
def load_csv_data(file_paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:
    data = {}
    for tf, path in file_paths.items():
        df = load_csv(path)  # load_csv ‡∏ï‡∏±‡πâ‡∏á index_col='time'
        data[tf] = df.sort_index()  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á index (DatetimeIndex) ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢->‡∏°‡∏≤‡∏Å
    return data

def create_features(df: pd.DataFrame, dfs_other: dict[str, pd.DataFrame]) -> pd.DataFrame:
    if 'time' in df.columns:
        df['time'] = pd.to_datetime(df['time'])
        df.set_index('time', inplace=True)

    # 2) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• M15/H1 ‡πÉ‡∏´‡πâ merge ‡∏Å‡πà‡∏≠‡∏ô
    if 'M15' in dfs_other and 'H1' in dfs_other:
        df = merge_timeframes(df, [dfs_other['M15'], dfs_other['H1']])

    # 3) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicator ‡∏´‡∏•‡∏±‡∏Å
    df = calculate_indicators(df)

    # 4) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì support/resistance
    df = calculate_support_resistance(df, window=20)

    # 5) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì volatility
    df = add_volatility_features(df)

    # 6) ‡∏™‡∏£‡πâ‡∏≤‡∏á lagged features
    df = add_lagged_features(df, cols=['RSI','MACD'], lags=[1,2,3])

    # 7) ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå confirm entry
    df = add_confirm_entry_feature(df)

    # 8) dropna ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢
    df = df.dropna()
    return df

def prepare_datasets(data: dict) -> list:
    """
    ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training/validation
    - ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 'M5' ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á label ‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏á MAX_HOLD_PERIOD ‡πÅ‡∏ó‡πà‡∏á
    - ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î‡∏î‡πâ‡∏ß‡∏¢ TimeSeriesSplit
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ list ‡∏Ç‡∏≠‡∏á tuples: (X_train, X_val, y_train, y_val)
    """
    df = create_features(data['M5'])
    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå target ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á X, y
    X = df.drop(columns=['close'])
    y = (df['close'].shift(-MAX_HOLD_PERIOD) > df['close']).astype(int)

    # ‡∏Å‡∏≥‡∏à‡∏±‡∏î‡πÅ‡∏ñ‡∏ß‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà label ‡πÄ‡∏õ‡πá‡∏ô NaN
    valid_idx = y.dropna().index
    X, y = X.loc[valid_idx], y.loc[valid_idx]

    # ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤
    tscv = TimeSeriesSplit(n_splits=3)
    splits = []
    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        splits.append((X_train, X_val, y_train, y_val))

    return splits

# üßÆ Indicator Calculation
def calculate_indicators(df: pd.DataFrame) -> pd.DataFrame:
    df['RSI']           = talib.RSI(df['close'], timeperiod=14)
    df['EMA_10']        = talib.EMA(df['close'], timeperiod=10)
    df['EMA_50']        = talib.EMA(df['close'], timeperiod=50)
    df['EMA_Crossover'] = df['EMA_10'] - df['EMA_50']
    df['MACD'], df['MACD_signal'], _ = talib.MACD(df['close'])
    df['ATR']           = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)
    df['OBV']           = talib.OBV(df['close'], df.get('tick_volume', df.get('volume', np.zeros(len(df)))))
    df['ADX']           = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)

    bb_u, bb_m, bb_l    = talib.BBANDS(df['close'], timeperiod=20, nbdevup=2, nbdevdn=2)
    df['BB_upper']      = bb_u
    df['BB_middle']     = bb_m
    df['BB_lower']      = bb_l
    df['BB_width']      = bb_u - bb_l
    df['CCI']           = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)

    df['close_change']  = df['close'].pct_change()
    df['hour']          = df.index.hour
    df['day_of_week']   = df.index.dayofweek

    # üî• ‡πÄ‡∏û‡∏¥‡πà‡∏° Volatility Indicators
    df['HistVol']       = df['close'].rolling(10).std()
    df['GK_vol']        = 0.5 * (np.log(df['high'] / df['low']) ** 2) - \
                          (2 * np.log(2) - 1) * (np.log(df['close'] / df['open']) ** 2)
    df['Parkinson_vol'] = (1 / (4 * np.log(2))) * ((np.log(df['high'] / df['low'])) ** 2)

    return df

# üìà Support / Resistance
def calculate_support_resistance(df: pd.DataFrame, window: int = 20) -> pd.DataFrame:
    df['support']    = df['low'].rolling(window=window, min_periods=1).min()
    df['resistance'] = df['high'].rolling(window=window, min_periods=1).max()
    return df

def add_volatility_features(df: pd.DataFrame) -> pd.DataFrame:
    # ‡∏≠‡∏≤‡∏à‡∏£‡∏ß‡∏° Garman-Klass, Parkinson, ATR (‡∏ã‡∏∂‡πà‡∏á ATR ‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏¥‡∏°), plus rolling std
    df['ATR'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)
    df['HistVol_10'] = df['close'].rolling(window=10).std()
    df['GK_vol'] = 0.5 * (np.log(df['high'] / df['low'])**2) - (2*np.log(2)-1)*(np.log(df['close']/df['open'])**2)
    df['Parkinson_vol'] = (1/(4*np.log(2))) * (np.log(df['high']/df['low'])**2)
    return df

def add_lagged_features(df: pd.DataFrame, cols: list[str], lags: list[int]) -> pd.DataFrame:
    for col in cols:
        for lag in lags:
            df[f"{col}_lag{lag}"] = df[col].shift(lag)
    return df

# üîó Merge Multi-Timeframe
def merge_timeframes(df_main, dfs_other, suffixes=['M15', 'H1']):
    df_merged = df_main.copy()
    for df, suffix in zip(dfs_other, suffixes):
        df = df.copy()
        df.columns = [f"{col}_{suffix}" for col in df.columns]
        df_merged = df_merged.join(df, how='left').fillna(method='ffill')
    return df_merged

# üîç Add Regime, Orderbook, Divergence, Momentum
def add_additional_features(df: pd.DataFrame) -> pd.DataFrame:
    df['regime_trend'] = (df['ADX'] > 25).astype(int)
    df['momentum_3'] = df['close'].pct_change(3)
    df['momentum_5'] = df['close'].pct_change(5)
    df['lag1_close'] = df['close'].shift(1)

    # RSI Divergence & EMA Cross TF
    if 'RSI_H1' in df.columns and 'RSI_M15' in df.columns:
        df['RSI_Divergence_H1_M15'] = df['RSI_H1'] - df['RSI_M15']
    if 'EMA_10_H1' in df.columns and 'EMA_10_M15' in df.columns:
        df['EMA_Cross_H1_M15'] = df['EMA_10_H1'] - df['EMA_10_M15']

    df['volatility_5'] = df['close'].rolling(5).std()
    df['RSI_lag1']     = df['RSI'].shift(1)
    df['MACD_lag1']    = df['MACD'].shift(1)
    return df.dropna()

# üìâ Orderbook Feature
def add_orderbook_features(df: pd.DataFrame, symbol: str) -> pd.DataFrame:
    book = mt5.market_book_get(symbol)
    if not book:
        df['order_imbalance'] = 0.0
        return df
    bids = sum(l.volume for l in book if l.type == mt5.BOOK_TYPE_BUY)
    asks = sum(l.volume for l in book if l.type == mt5.BOOK_TYPE_SELL)
    df['order_imbalance'] = (bids - asks) / max(bids + asks, 1)
    return df

def create_labels_from_price(
        data_pca: np.ndarray,
        prices: np.ndarray,
        look_back: int,
        hold_bars: int,
        thr_short: float,
        thr_long: float
    ) -> tuple[np.ndarray, np.ndarray]:
    X_list, y_list = [], []
    n = len(prices)
    assert data_pca.shape[0] == n, "data_pca ‡πÅ‡∏•‡∏∞ prices ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô"
    max_i = n - look_back - hold_bars
    for i in range(max_i):
        seq = data_pca[i : i + look_back]  # shape=(look_back, n_features_pca)
        ret = prices[i + look_back + hold_bars] / prices[i + look_back] - 1.0

        if ret > thr_long:
            lbl = 2
        elif ret < -thr_short:
            lbl = 0
        else:
            lbl = 1

        X_list.append(seq)
        y_list.append(lbl)

    X = np.array(X_list)                 # (n_seq, look_back, n_features_pca)
    y = to_categorical(y_list, num_classes=3)
    return X, y

def create_labels_from_price_nonzero(
        data_pca: np.ndarray,
        prices: np.ndarray,
        look_back: int,
        hold_bars: int,
        thr_short: float,
        thr_long: float
    ) -> tuple[np.ndarray, np.ndarray]:
    n = len(prices)
    max_i = n - look_back - hold_bars

    # 1) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì forward_returns ‡πÅ‡∏•‡∏∞ mask_nonzero
    forward_returns = np.empty(max_i, dtype=np.float32)
    for i in range(max_i):
        forward_returns[i] = prices[i + look_back + hold_bars] / prices[i + look_back] - 1.0

    # mask of positions where return != 0
    mask_nonzero = forward_returns != 0.0
    indices = np.nonzero(mask_nonzero)[0]  # array ‡∏Ç‡∏≠‡∏á i ‡∏ó‡∏µ‡πà ret != 0

    X_list, y_list = [], []
    for i in indices:
        seq = data_pca[i : i + look_back]  # shape = (look_back, n_features_pca)
        ret = forward_returns[i]
        if ret > thr_long:
            lbl = 2
        elif ret < -thr_short:
            lbl = 0
        else:
            lbl = 1
        X_list.append(seq)
        y_list.append(lbl)

    X = np.array(X_list)
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á hold/long/short ‡πÄ‡∏•‡∏¢ ‡∏≠‡∏¢‡πà‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô error ‡πÅ‡∏ï‡πà return empty
    if len(y_list) == 0:
        return np.empty((0, look_back, data_pca.shape[1])), np.empty((0, 3))
    y = to_categorical(y_list, num_classes=3)
    return X, y

# =============================================
# üéØ Section 3: Dataset, Threshold, Augmentation
# =============================================

from imblearn.over_sampling import SMOTE
from keras.utils import to_categorical
from typing import Tuple, Union, Optional

# üéØ Threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î long/short
def calculate_trade_threshold(
    df: pd.DataFrame,
    atr_period: int = 14,
    multiplier: float = 0.5
) -> float:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì threshold ‡∏î‡πâ‡∏ß‡∏¢ ATR ‡πÅ‡∏ö‡∏ö rolling mean
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ float ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: avg_ATR * multiplier
    """
    # 1) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ATR ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (high, low, close ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ)
    atr = talib.ATR(
        df['high'],
        df['low'],
        df['close'],
        timeperiod=atr_period
    )
    # 2) ‡πÄ‡∏≠‡∏≤ ATR ‡∏°‡∏≤‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ rolling mean ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á atr_period ‡πÅ‡∏ó‡πà‡∏á
    avg_atr = atr.rolling(window=atr_period, min_periods=1).mean().iloc[-1]
    # 3) ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ threshold
    return avg_atr * multiplier

# üéØ Create Classification Dataset
def create_dataset_classification(
    data: np.ndarray,
    look_back: int,
    threshold: Union[float, Tuple[float,float]]
):
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô tuple ‡πÅ‡∏à‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    if isinstance(threshold, (list,tuple,np.ndarray)):
        thr_short, thr_long = threshold
    else:
        thr_short = thr_long = threshold

    X, y = [], []
    for i in range(len(data) - look_back - 1):
        seq = data[i : i+look_back]
        ret = data[i+look_back+1, 0] - data[i+look_back, 0]
        if   ret >  thr_long: lbl = 2
        elif ret < -thr_short: lbl = 0
        else:                  lbl = 1
        X.append(seq)
        y.append(lbl)
    X = np.array(X)
    y = to_categorical(y, num_classes=3)
    return X, y

# üîÅ Data Augmentation (‡πÄ‡∏û‡∏¥‡πà‡∏° noise)
def augment_data(X: np.ndarray, noise_level: float = 0.001) -> np.ndarray:
    noise = np.random.normal(0, noise_level, X.shape)
    return np.concatenate([X, X + noise], axis=0)

# ‚öñÔ∏è Oversample dataset ‡πÉ‡∏´‡πâ balance
def oversample_dataset(X: np.ndarray, y: np.ndarray, look_back: int = LOOK_BACK):
    y_labels = np.argmax(y, axis=1)
    if len(np.unique(y_labels)) < 2:
        return X, y  # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏∞ oversample
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î k_neighbors = min(5, smallest_class_count-1)
    counts = np.bincount(y_labels)
    k = max(1, min(5, counts.min() - 1))
    sm = SMOTE(sampling_strategy='auto', k_neighbors=k, random_state=42)
    X_flat = X.reshape(X.shape[0], -1)
    X_res, y_res = sm.fit_resample(X_flat, y_labels)
    X_res = X_res.reshape(-1, look_back, X.shape[2])
    y_res = to_categorical(y_res, num_classes=3)
    return X_res, y_res

# ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠‡πÑ‡∏´‡∏°
def check_dataset_sufficiency(df: pd.DataFrame, look_back: int, hold_bars: int, min_trades: int):
    required = (look_back + hold_bars + 1) * min_trades
    avail = len(df)
    logger.info(f"Dataset check: need ‚â•{required}, have {avail}")
    if avail < required:
        logger.error(f"‚ùå Insufficient data: {avail} < {required}")
        sys.exit(1)

# ==================================
# üß† Section 4: Model Builders
# ==================================
try:
    # ‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ TensorFlow ‚â•2.x
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import (
        Layer, Input, LSTM, Dense, Dropout, Bidirectional,
        Conv1D, MaxPooling1D, GlobalAveragePooling1D,
        MultiHeadAttention, LayerNormalization, Add
    )
    from tensorflow.keras.regularizers import l2
except ModuleNotFoundError:
    # fallback ‡πÑ‡∏õ standalone Keras
    from keras.models import Sequential, Model
    from keras.layers import (
        Layer, Input, LSTM, Dense, Dropout, Bidirectional,
        Conv1D, MaxPooling1D, GlobalAveragePooling1D,
        MultiHeadAttention, LayerNormalization
    )
    from keras.regularizers import l2
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from tcn import TCN

# ‚úÖ Custom Attention Layer
class AttentionLayer(Layer):
    def build(self, input_shape):
        # input_shape = (batch_size, timesteps, features)
        self.W = self.add_weight(
            shape=(input_shape[-1], 1),   # features √ó 1
            initializer="normal",
            name="att_weight"
        )
        self.b = self.add_weight(
            shape=(1, 1),                 # bias scalar (broadcast ‡πÑ‡∏î‡πâ)
            initializer="zeros",
            name="att_bias"
        )
        super().build(input_shape)

    def call(self, x):
        # x: (batch, timesteps, features)
        e = K.tanh(K.dot(x, self.W) + self.b)  # (batch, timesteps, 1)
        a = K.softmax(e, axis=1)               # normalize over timesteps
        # sum weighted features ‡∏ï‡∏≤‡∏° timesteps ‚Üí shape = (batch, features)
        return K.sum(x * a, axis=1)

    def compute_output_shape(self, input_shape):
        # output shape = (batch, features)
        return (input_shape[0], input_shape[-1])

# ‚úÖ LSTM + Attention Deep Model
def build_model_lstm_att_hp(look_back: int, n_features: int) -> Model:
    inp = Input(shape=(look_back, n_features))
    x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001)))(inp)
    x = Dropout(0.5)(x)
    x = Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001)))(x)
    x = Dropout(0.4)(x)
    x = Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=l2(0.001)))(x)
    x = AttentionLayer()(x)
    out = Dense(3, activation='softmax')(x)
    model = Model(inputs=inp, outputs=out)
    # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ mixed_precision
    base_opt = Adam(learning_rate=1e-4)
    opt = LossScaleOptimizer(base_opt)
    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# ‚úÖ CNN + LSTM
def build_model_cnn_lstm(look_back: int, n_features: int) -> Model:
    inp = Input(shape=(look_back, n_features))
    x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(inp)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.4)(x)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ look_back=30 ‚Üí ‡∏´‡∏•‡∏±‡∏á pool ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ length=14 (‡∏ñ‡πâ‡∏≤ valid padding)
    x = LSTM(64, return_sequences=False, kernel_regularizer=l2(0.001))(x)
    x = Dropout(0.3)(x)
    out = Dense(3, activation='softmax')(x)
    model = Model(inputs=inp, outputs=out)
    base_opt = Adam(learning_rate=1e-4)
    opt = LossScaleOptimizer(base_opt)
    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def build_model_tcn(look_back, n_features, filters=64, kernel_size=3):
    inp = Input(shape=(look_back, n_features), dtype="float16")
    x   = inp
    # ‡πÉ‡∏ä‡πâ 3 ‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå causal conv + residual
    for d in [1,2,4]:
        y = Conv1D(filters, kernel_size,
                   dilation_rate=d,
                   padding="causal",
                   activation="relu")(x)
        y = Dropout(0.4)(y)
        # residual
        if y.shape[-1] != x.shape[-1]:
            x_proj = Dense(filters)(x)
        else:
            x_proj = x
        x = tf.keras.layers.Add()([x_proj, y])

    x   = GlobalAveragePooling1D()(x)
    out = Dense(3, activation="softmax", dtype="float32")(x)  # cast back to float32
    base_opt = Adam(1e-4)
    opt      = LossScaleOptimizer(base_opt)
    model = Model(inp, out)
    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

def build_model_transformer(
    look_back, n_features,
    head_size=64, num_heads=4,
    ff_dim=128, dropout=0.1, n_blocks=4
):
    inp = Input(shape=(look_back, n_features), dtype="float16")
    x   = inp
    for _ in range(n_blocks):
        # multi-head attention
        y = MultiHeadAttention(
                key_dim=head_size, num_heads=num_heads, dropout=dropout
            )(x, x)
        y = Dropout(dropout)(y)
        # residual + norm
        res = tf.cast(x, y.dtype)
        x   = LayerNormalization(epsilon=1e-4)(res + y)

        # FFN
        z = Dense(ff_dim, activation="relu")(x)
        z = Dropout(dropout)(z)
        z = Dense(n_features)(z)
        res2 = tf.cast(x, z.dtype)
        x    = LayerNormalization(epsilon=1e-4)(res2 + z)

    x   = GlobalAveragePooling1D()(x)
    x   = Dropout(dropout)(x)
    out = Dense(3, activation="softmax", dtype="float32")(x)
    base_opt = Adam(1e-4)
    opt      = LossScaleOptimizer(base_opt)
    model = Model(inp, out)
    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

def build_model_tft(
    look_back, n_features,
    head_size=64, num_heads=4,
    ff_dim=128, dropout=0.1, n_blocks=3
):
    inp = Input(shape=(look_back, n_features), dtype="float16")
    x   = inp
    for _ in range(n_blocks):
        # attention block
        y = MultiHeadAttention(
                key_dim=head_size, num_heads=num_heads, dropout=dropout
            )(x, x)
        y = Dropout(dropout)(y)
        res = tf.cast(x, y.dtype)
        x   = LayerNormalization(epsilon=1e-4)(res + y)

        # FFN
        z = Dense(ff_dim, activation="relu")(x)
        z = Dropout(dropout)(z)
        z = Dense(n_features)(z)
        res2 = tf.cast(x, z.dtype)
        x    = LayerNormalization(epsilon=1e-4)(res2 + z)

    # ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ LSTM
    x   = LSTM(64, return_sequences=False)(x)
    x   = Dropout(dropout)(x)
    out = Dense(3, activation="softmax", dtype="float32")(x)
    base_opt = Adam(1e-4)
    opt      = LossScaleOptimizer(base_opt)
    model = Model(inp, out)
    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

# ‚úÖ Meta-Model (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ensemble)
def build_meta_model(input_dim: int) -> Model:
    model = Sequential([
        Dense(16, activation='relu', input_shape=(input_dim,)),
        Dropout(0.2),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# ‚úÖ RF & XGB
def build_rf_model() -> RandomForestClassifier:
    return RandomForestClassifier(
        n_estimators=100,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )

def build_xgb_model(n_classes: int = 3) -> xgb.XGBClassifier:
    return xgb.XGBClassifier(
        n_estimators      = 100,
        objective         = "multi:softprob",  # ‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô multi-class
        num_class         = n_classes,         # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á
        use_label_encoder = False,
        eval_metric       = "mlogloss",
        random_state      = 42
    )

# ‚úÖ ‡∏£‡∏ß‡∏°‡∏ú‡∏• prediction ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å base model
def get_base_predictions(models, X, rf_model=None, batch_size=64):
    """
    Predict ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô batch ‡∏¢‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î footprint ‡∏Ç‡∏≠‡∏á GPU memory
    ‡πÅ‡∏•‡πâ‡∏ß clear_session ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    preds = []
    # 1) ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡∏•‡∏∞ batch
    for idx, m in enumerate(models):
        if hasattr(m, 'predict'):
            # sklearn model
            arr = m.predict(X)
        else:
            # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ predict ‡∏ö‡∏ô CPU ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á TensorDataset error
            with tf.device('/CPU:0'):
                arr = m.predict(X, batch_size=batch_size, verbose=0)
            if arr.ndim == 1:
                arr = arr.reshape(-1, 1)
        print(f"model #{idx} ({type(m).__name__}): {arr.shape}")
        preds.append(arr)

    # 2) ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ RF ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if rf_model:
        flat = X.reshape(X.shape[0], -1)
        if hasattr(rf_model, 'predict_proba'):
            arr = rf_model.predict_proba(flat)
        else:
            # fallback: reshape ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå 1-D ‡πÄ‡∏õ‡πá‡∏ô 2-D
            arr = rf_model.predict(flat).reshape(-1, 1)
        print(f"rf_model: {arr.shape}")
        preds.append(arr)

    # 3) ‡∏ï‡∏±‡∏î‡∏ó‡∏∏‡∏Å‡∏≠‡∏≤‡∏£‡πå‡πÄ‡∏£‡∏¢‡πå‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô (min_n)
    min_n = min(p.shape[0] for p in preds)
    preds = [p if p.ndim==2 else p.reshape(-1,1) for p in preds]

    K.clear_session()
    gc.collect()
    return np.concatenate(preds, axis=1)

# ============================================
# üöÄ Section 5: Tuning, SHAP, Drift, Save
# ============================================
import matplotlib.pyplot as plt
from bayes_opt import BayesianOptimization
from sklearn.model_selection import TimeSeriesSplit
from tensorflow.keras.callbacks import EarlyStopping
import shap

# üîÅ Tuning TCN
def tune_tcn(X, y, look_back: int, pbounds=None):
    if pbounds is None:
        pbounds = {'filters': (16, 128), 'kernel': (2, 8)}

    y_lbl = np.argmax(y, axis=1)

    def cv(filters, kernel):
        K.clear_session()
        gc.collect()

        model = build_model_tcn(
            look_back, X.shape[2],
            filters=int(filters),
            kernel_size=int(kernel)
        )

        es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
        h = model.fit(
            X, y_lbl,
            validation_split=0.2,
            epochs=10,
            batch_size=64,
            callbacks=[es],
            verbose=0
        )
        return max(h.history['val_accuracy'])

    opt = BayesianOptimization(f=cv, pbounds=pbounds, random_state=42)
    opt.maximize(init_points=5, n_iter=10)
    logger.info("‚úÖ TCN tuning done: %s", opt.max)
    return opt.max['params']

def tune_tft(X, y, look_back: int, pbounds=None):
    if pbounds is None:
        pbounds = {
            'ff_dim':    (64, 256),
            'head_size': (16,  64),
            'dropout':   (0.0,  0.5)
        }

    # 1) integer labels
    y_lbl = np.argmax(y, axis=1)
    # 2) float32 for Keras
    X = np.asarray(X, dtype=np.float32)

    # 3) cv must accept (ff_dim, head_size, dropout)
    def cv(ff_dim, head_size, dropout):
        K.clear_session()
        gc.collect()

        model = build_model_tft(
            look_back  = look_back,
            n_features = X.shape[2],
            head_size  = int(head_size),
            ff_dim     = int(ff_dim),
            dropout    = float(dropout),   # <‚Äî now uses the argument
            n_blocks   = 3,
            num_heads  = 4
        )

        es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
        tscv = TimeSeriesSplit(n_splits=3)
        vals = []
        for tr_idx, va_idx in tscv.split(X):
            X_tr, X_va = X[tr_idx], X[va_idx]
            y_tr, y_va = y_lbl[tr_idx], y_lbl[va_idx]
            h = model.fit(
                X_tr, y_tr,
                validation_data=(X_va, y_va),
                epochs=10,
                batch_size=64,
                callbacks=[es],
                verbose=0
            )
            vals.append(max(h.history['val_accuracy']))
        return float(np.mean(vals))

    # 4) now BO will pass dropout
    opt = BayesianOptimization(f=cv, pbounds=pbounds, random_state=42)
    opt.maximize(init_points=3, n_iter=5)

    print("‚úÖ TFT tuning done:", opt.max)
    return opt.max['params']

def explain_with_shap(model, X, feat_names, out_path: Optional[str] = None):
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Explainer ‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•
    if hasattr(model, 'feature_importances_'):
        expl = shap.TreeExplainer(model)
    else:
        expl = shap.DeepExplainer(model, X[:50])
    subset = X[50:100]
    sv = expl.shap_values(subset)
    # ‡∏ß‡∏≤‡∏î summary plot
    shap.summary_plot(sv, features=subset, feature_names=feat_names, show=False)
    if out_path:
        plt.savefig(out_path, bbox_inches='tight')
    else:
        plt.show()

# üîÑ Concept Drift Detection
def detect_concept_drift(ref: np.ndarray, new: np.ndarray, alpha=0.01):
    """
    ‡πÉ‡∏ä‡πâ KS-test ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à distribution drift ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature
    ‡∏Ñ‡∏∑‡∏ô True ‡∏ñ‡πâ‡∏≤ severity ‚â• 20%
    """
    drift_count = 0
    n_feat = ref.shape[1]
    for i in range(n_feat):
        _, p = ks_2samp(ref[:, i], new[:, i])
        if p < alpha:
            drift_count += 1
    severity = drift_count / n_feat
    logger.info(f"üìâ Drift severity: {severity:.2%}")
    return severity > 0.2

def auto_retrain_if_drift(model, refX, newX, newY):
    if detect_concept_drift(refX, newX):
        # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°
        model.save('backup_model.h5')
        logger.warning("‚ö†Ô∏è Concept drift detected ‚Üí retraining")
        history = model.fit(newX, newY,
                            epochs=10, batch_size=64,
                            validation_split=0.2, verbose=1)
        val_acc = max(history.history.get('val_accuracy', [0]))
        logger.info(f"üîÑ Retrained, best val_acc={val_acc:.2%}")

# üíæ Save Model + Scaler + PCA + Tree
def save_model_bundle(models: dict, folder: str = 'models'):
    os.makedirs(folder, exist_ok=True)
    for name, obj in models.items():
        base = os.path.join(folder, name)
        if hasattr(obj, 'save'):  # Keras
            obj.save(base + ".h5")
        elif isinstance(obj, sklearn.base.BaseEstimator):
            joblib.dump(obj, base + ".pkl")
        else:
            with open(base + ".bin", "wb") as f:
                pickle.dump(obj, f)
        logger.info("üì¶ Saved: %s", base)

# ================================================
# üéØ FINAL: main() ‚Äî ‡∏ù‡∏∂‡∏Å EA22 + EA27 ‡πÅ‡∏•‡∏∞ Save
# ================================================
import pickle
import MetaTrader5 as mt5
import time
import numpy as np
import pandas as pd
import joblib
import warnings
import gc
import keras_tuner as kt
import xgboost as xgb
import tensorflow as tf
from hmmlearn.hmm import GaussianHMM
from typing import Optional
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, StratifiedKFold
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Input, Conv1D, MaxPooling1D, Flatten, Add
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
from sklearn.feature_selection import mutual_info_classif
from tensorflow.keras import regularizers
from tensorflow.keras import backend as K
from collections import OrderedDict
from tqdm import tqdm
from tqdm.keras import TqdmCallback
from tensorflow.keras.callbacks import CSVLogger
from sklearn.base import BaseEstimator
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import custom_object_scope
from tcn import TCN
from imblearn.over_sampling import RandomOverSampler
from sklearn.utils.class_weight import compute_class_weight
from typing import Dict
from tensorflow.keras.callbacks import TensorBoard
from xgboost import XGBClassifier
from sklearn.utils import compute_class_weight
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import confusion_matrix
from sklearn.isotonic import IsotonicRegression

# üß† Load all model builder functions
# Assumes functions like build_model_lstm_att_hp, build_model_cnn_lstm etc. are defined elsewhere
        
# === Tick-level Volume Profile ===
def get_volume_profile(symbol: str, n_bins: int = 20, duration_sec: int = 300):
    now = time.time()
    ticks = mt5.copy_ticks_from(symbol, now - duration_sec, duration_sec, mt5.COPY_TICKS_ALL)
    if ticks is None or len(ticks) == 0:
        return np.zeros(n_bins)
    df = pd.DataFrame(ticks)
    price_bins = np.linspace(df['last'].min(), df['last'].max(), n_bins + 1)
    df['bin'] = np.digitize(df['last'], price_bins)
    vol_profile = df.groupby('bin')['volume'].sum().reindex(range(1, n_bins + 1), fill_value=0).values
    return vol_profile / (vol_profile.sum() + 1e-8)

# === Orderbook Depth Feature ===
def get_orderbook_features(symbol: str, max_levels: int = 10):
    book = mt5.market_book_get(symbol)
    if not book:
        return np.zeros(max_levels + 3)
    bids = [l for l in book if l.type == mt5.BOOK_TYPE_BUY][:max_levels]
    asks = [l for l in book if l.type == mt5.BOOK_TYPE_SELL][:max_levels]
    bid_vols = np.array([l.volume for l in bids] + [0] * (max_levels - len(bids)))
    ask_vols = np.array([l.volume for l in asks] + [0] * (max_levels - len(asks)))
    imbalance_per_level = (bid_vols - ask_vols) / (bid_vols + ask_vols + 1e-8)
    spread = asks[0].price - bids[0].price if bids and asks else 0
    total_bid = bid_vols.sum()
    total_ask = ask_vols.sum()
    return np.concatenate([imbalance_per_level, [total_bid, total_ask, spread]])

# === Add to df ===
def enrich_df_with_tick_orderbook(df: pd.DataFrame, symbol: str):
    vp = get_volume_profile(symbol)
    ob = get_orderbook_features(symbol)
    for i in range(len(vp)):
        df[f'vp_bin_{i+1}'] = vp[i]
    for i in range(len(ob)-3):
        df[f'imb_{i+1}'] = ob[i]
    df['total_bid'] = ob[-3]
    df['total_ask'] = ob[-2]
    df['spread'] = ob[-1]
    return df

# === Feature Selection ===
def select_features_by_mutual_info(X: np.ndarray, y: np.ndarray, k: int = 30):
    y_labels = np.argmax(y, axis=1) if len(y.shape) > 1 else y
    scores = mutual_info_classif(X, y_labels)
    top_k_idx = np.argsort(scores)[-k:]
    return X[:, top_k_idx], top_k_idx

# === Autoencoder Feature Extractor ===
def build_autoencoder(input_dim: int, encoding_dim: int = 32) -> Model:
    inp = Input(shape=(input_dim,))
    encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(1e-4))(inp)
    decoded = Dense(input_dim, activation='linear')(encoded)
    autoencoder = Model(inp, decoded)
    autoencoder.compile(optimizer='adam', loss='mse')
    encoder = Model(inp, encoded)
    return autoencoder, encoder

# === Meta-Model Builder ===
def build_meta_model(input_dim: int):
    model = Sequential([
        Dense(16, activation='relu', input_dim=input_dim),
        Dropout(0.2),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# === Deep Model (LSTM) ===
def build_lstm_classifier(input_dim):
    model = Sequential([
        Input(shape=(1, input_dim)),
        LSTM(64),
        Dropout(0.3),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# === Deep Model (CNN) ===
def build_cnn_classifier(input_dim):
    model = Sequential([
        Input(shape=(1, input_dim)),
        Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01)),
        MaxPooling1D(pool_size=1),
        Flatten(),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# === Mock save_model_bundle ===
def save_model_bundle(models: dict, folder: str = 'models'):
    os.makedirs(folder, exist_ok=True)
    for name, obj in models.items():
        path = os.path.join(folder, name)
        if hasattr(obj, 'save'):
            # Keras models
            obj.save(path + ".h5")
        elif isinstance(obj, BaseEstimator):
            # sklearn models
            joblib.dump(obj, path + ".pkl")
        else:
            # objects ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            with open(path + ".bin", "wb") as f:
                pickle.dump(obj, f)
        logger.info("üì¶ Saved artifact: %s", path)

# === Stub utils ===
def load_csv(path: str) -> pd.DataFrame:
    # 1) ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå time ‡πÄ‡∏õ‡πá‡∏ô datetime  
    # 2) ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô index ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ downstream ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏ä‡πâ df.index.hour, df.index.dayofweek ‡πÑ‡∏î‡πâ  
    return pd.read_csv(path, parse_dates=['time'], index_col='time')

# === Multi-Horizon Label Generator ===
def generate_multi_horizon_labels(data: np.ndarray, look_back: int, horizons=[1, 3, 5]):
    X, y = [], []
    for i in range(len(data) - look_back - max(horizons)):
        X.append(data[i:i+look_back])
        future = [data[i+look_back+h, 0] - data[i+look_back, 0] for h in horizons]
        y.append(future)
    return np.array(X), np.array(y)

def detect_market_regime(prices: np.ndarray, n_states: int = 3) -> np.ndarray:
    """
    ‡∏Ñ‡∏∑‡∏ô array ‡∏Ç‡∏≠‡∏á regime index ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ bar
    ‡∏ñ‡πâ‡∏≤ len(prices)=N ‡∏Ñ‡∏∑‡∏ô regimes.shape = N
    """
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô log-returns
    returns = np.diff(np.log(prices + 1e-8)).reshape(-1, 1)  # shape=(N-1,1)
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å HMM
    model = GaussianHMM(n_components=n_states, covariance_type="diag", n_iter=100)
    model.fit(returns)                   # ‡πÅ‡∏¢‡∏Å fit ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    regimes = model.predict(returns)     # ‡πÅ‡∏•‡πâ‡∏ß predict
    # pad ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤ 1 ‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö prices
    regimes = np.insert(regimes, 0, regimes[0])
    return regimes  # shape=(N,)

def prepare_base_dataframe(path: str, symbol: str, dfs_other: Optional[dict[str, pd.DataFrame]] = None) -> pd.DataFrame:
    # 1) ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV
    df = load_csv(path)  # index_col='time'
    
    # 2) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å
    if dfs_other:
        df = create_features(df, dfs_other)
    else:
        df = create_features(df, {})

    # 3) ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• tick-level & orderbook
    df = enrich_df_with_tick_orderbook(df, symbol)
    
    # 4) Detect regime ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏•‡∏±‡∏á enrich ‡πÄ‡∏™‡∏£‡πá‡∏à
    prices = df['close'].values
    df['regime'] = detect_market_regime(prices, n_states=3)

    return df
    
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report

def categorical_focal_loss(gamma=2., alpha=0.25):
    def loss(y_true, y_pred):
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1-1e-7)
        cross_entropy = -y_true * tf.math.log(y_pred)
        weight = alpha * tf.pow(1 - y_pred, gamma)
        return tf.reduce_sum(weight * cross_entropy, axis=1)
    return loss

def cross_validate_ea22(
    X: np.ndarray,
    y: np.ndarray,
    build_base_fns: list,
    build_rf_fn,
    build_meta_fn,
    look_back: int,
    batch_size: int = 64,
    epochs: int = 10,
    n_splits: int = 5,
    long_boost: float = 1.5       # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™ Long
):
    y_lbl = np.argmax(y, axis=1)
    skf   = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

    accs, f1_hold_list, f1_macro_list = [], [], []

    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y_lbl), start=1):
        print(f"\n[EA22 CV] Fold {fold}/{n_splits}")
        X_tr, X_va = X[tr_idx], X[va_idx]
        y_tr_lbl   = y_lbl[tr_idx]
        y_va_lbl   = y_lbl[va_idx]

        # 1) Train base deep models
        trained = []
        for build_fn in build_base_fns:
            m = build_fn(look_back, X_tr.shape[2])
            m.fit(X_tr, y_tr_lbl, epochs=epochs, batch_size=batch_size, verbose=0)
            trained.append(m)

        # 2) Train RF
        rf = build_rf_fn()
        rf.fit(X_tr.reshape(len(X_tr), -1), y_tr_lbl)

        # 3) ‡∏™‡∏£‡πâ‡∏≤‡∏á meta-inputs
        meta_X_tr = get_base_predictions(trained, X_tr, rf_model=rf, batch_size=batch_size)
        meta_X_va = get_base_predictions(trained, X_va, rf_model=rf, batch_size=batch_size)

        # 4) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° class_weight_meta + boost ‡∏Ñ‡∏•‡∏≤‡∏™ Long (2)
        classes_meta = np.unique(y_tr_lbl)
        cw_vals      = compute_class_weight('balanced', classes=classes_meta, y=y_tr_lbl)
        class_weight_meta = {int(c): float(w) for c,w in zip(classes_meta, cw_vals)}
        class_weight_meta.setdefault(2, 1.0)
        class_weight_meta[2] *= long_boost
        print("  class_weight_meta:", class_weight_meta)

        # 5) Train meta-model
        meta = build_meta_fn(input_dim=meta_X_tr.shape[1])
        meta.compile(optimizer=Adam(1e-4),
                     loss='sparse_categorical_crossentropy',
                     metrics=['accuracy'])
        meta.fit(meta_X_tr, y_tr_lbl,
                 epochs=epochs,
                 batch_size=batch_size,
                 class_weight=class_weight_meta,
                 verbose=0)

        # ---- Calibration (Isotonic) ----
        prob_tr   = meta.predict(meta_X_tr)       # shape=(n_tr,3)
        P_hold_tr = prob_tr[:, 1]
        y_hold_tr = (y_tr_lbl == 1).astype(int)

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á Hold ‡πÅ‡∏•‡∏∞ non-Hold ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å ‡∏à‡∏∂‡∏á calibrate
        if len(np.unique(y_hold_tr)) == 2:
            # 1) ‡∏Å‡∏£‡∏≠‡∏á NaN ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å P_hold_tr
            mask     = ~np.isnan(P_hold_tr)
            P_clean  = P_hold_tr[mask]
            y_clean  = y_hold_tr[mask]
            # 2) Fit isotonic
            iso      = IsotonicRegression(out_of_bounds='clip')
            iso.fit(P_clean, y_clean)
            # 3) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏£‡∏≠‡∏á/‡πÄ‡∏ï‡∏¥‡∏° NaN ‡πÅ‡∏•‡πâ‡∏ß predict
            def calibrate(p):
                # ‡πÅ‡∏ó‡∏ô NaN ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤ 0.5 ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ predict
                p2 = np.nan_to_num(p, nan=0.5)
                return iso.predict(p2)
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏° calibration
            def calibrate(p):
                # ‡πÅ‡∏Ñ‡πà‡πÄ‡∏ï‡∏¥‡∏° NaN ‡πÄ‡∏õ‡πá‡∏ô 0.5 ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô p ‡∏î‡∏¥‡∏ö
                return np.nan_to_num(p, nan=0.5)

        # ---- ‡πÉ‡∏ä‡πâ calibrate ‡πÄ‡∏ß‡∏•‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå ----
        prob_va    = meta.predict(meta_X_va)
        raw_hold   = prob_va[:, 1]
        P_hold_cal = calibrate(raw_hold)

        # ---- ‡∏´‡∏≤ optimal threshold ‡∏î‡πâ‡∏ß‡∏¢ F1-macro ----
        y_true      = y_va_lbl
        best_thr, best_f1m = 0.0, 0.0
        for thr in np.linspace(0.30, 0.60, 301):
            y_pred_thr = np.where(
                P_hold_cal > thr,
                1,
                np.argmax(prob_va, axis=1)
            )
            f1m = f1_score(y_true, y_pred_thr, average='macro')
            if f1m > best_f1m:
                best_f1m, best_thr = f1m, thr

        print(f"  best_hold_thr={best_thr:.6f}, F1-macro={best_f1m:.4f}")

        # ---- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ threshold ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡πÑ‡∏î‡πâ ----
        y_pred = np.where(P_hold_cal > best_thr,
                          1,
                          np.argmax(prob_va, axis=1))

        acc      = accuracy_score(y_true, y_pred)
        f1_hold  = f1_score(y_true, y_pred, labels=[1], average='macro')
        f1_macro = f1_score(y_true, y_pred, average='macro')

        print(f"  accuracy: {acc:.4f}, F1-Hold: {f1_hold:.4f}, F1-macro: {f1_macro:.4f}")
        print("  Confusion Matrix:")
        print(confusion_matrix(y_true, y_pred, labels=[0,1,2]))

        accs.append(acc)
        f1_hold_list.append(f1_hold)
        f1_macro_list.append(f1_macro)

        # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå memory
        from tensorflow.keras import backend as K
        import gc
        K.clear_session()
        gc.collect()

    print(f"\n[EA22 CV] Mean accuracy : {np.mean(accs):.4f} ¬± {np.std(accs):.4f}")
    print(f"[EA22 CV] Mean F1-Hold  : {np.mean(f1_hold_list):.4f} ¬± {np.std(f1_hold_list):.4f}")
    print(f"[EA22 CV] Mean F1-macro : {np.mean(f1_macro_list):.4f} ¬± {np.std(f1_macro_list):.4f}")

    return accs, f1_hold_list, f1_macro_list

def find_threshold_by_grid(
        df: pd.DataFrame,
        data_pca: np.ndarray,
        look_back: int,
        hold_bars: int,
        low: float = 0.0001,
        high: float = 0.005,
        step: float = 0.0001,
        target_hold_frac: tuple[float, float] = (0.10, 0.30)
    ) -> tuple[float, float]:
    prices = df['close'].values
    returns = pd.Series(prices).pct_change().dropna().values

    # ‡∏•‡∏π‡∏õ‡∏´‡∏≤ thr symmetric ‡∏ó‡∏µ‡πà Hold frac ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á target_hold_frac
    for thr in np.arange(low, high + 1e-9, step):
        X_tmp, y_tmp = create_labels_from_price(
            data_pca,
            prices,
            look_back=look_back,
            hold_bars=hold_bars,
            thr_short=thr,
            thr_long=thr
        )
        hold_frac = np.mean(np.argmax(y_tmp, axis=1) == 1)
        if target_hold_frac[0] <= hold_frac <= target_hold_frac[1]:
            print(f"Found thr={thr:.6f} ‚Üí Hold frac {hold_frac:.2%}")
            return thr, thr

    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô grid ‚Üí fallback ‡πÄ‡∏õ‡πá‡∏ô percentile
    p25, p75 = np.percentile(returns, [25, 75])
    thr_short, thr_long = abs(p25), abs(p75)
    if thr_short == 0 and thr_long == 0:
        base = np.std(returns)
    else:
        base = max(thr_short, thr_long)

    for factor in [1, 2, 5, 10]:
        tst = base * factor
        X_tmp, y_tmp = create_labels_from_price(
            data_pca,
            prices,
            look_back=look_back,
            hold_bars=hold_bars,
            thr_short=tst,
            thr_long=tst
        )
        hold_frac = np.mean(np.argmax(y_tmp, axis=1) == 1)
        if hold_frac > 0:
            print(f"Fallback std√ó{factor}: thr={tst:.6f} ‚Üí Hold frac {hold_frac:.2%}")
            return tst, tst

    print("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ Hold ‡πÄ‡∏•‡∏¢ ‚Üí return (0,0)")
    return 0.0, 0.0

def oversample_hold_only(
        X: np.ndarray,
        y: np.ndarray,
        hold_class: int = 1,
        target_frac: float = 0.12
    ) -> tuple[np.ndarray, np.ndarray]:
    labels = np.argmax(y, axis=1)
    counts = np.bincount(labels, minlength=y.shape[1])
    n_total = len(labels)
    n_hold  = counts[hold_class]
    n_desired_hold = int(target_frac * n_total)

    # ‡∏ñ‡πâ‡∏≤ Hold ‡∏°‡∏µ‡πÄ‡∏¢‡∏≠‡∏∞‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô target ‚Üí ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∑‡∏ô dataset ‡πÄ‡∏î‡∏¥‡∏°
    if n_hold >= n_desired_hold:
        return X, y

    # ‡∏Å‡∏£‡∏ì‡∏µ Hold ‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ target ‚Üí oversample
    from imblearn.over_sampling import RandomOverSampler
    from tensorflow.keras.utils import to_categorical

    ros = RandomOverSampler(
        sampling_strategy={hold_class: n_desired_hold},
        random_state=42
    )
    X_flat = X.reshape(len(X), -1)
    X_res_flat, y_res_lbls = ros.fit_resample(X_flat, labels)
    X_res = X_res_flat.reshape(-1, X.shape[1], X.shape[2])
    y_res = to_categorical(y_res_lbls, num_classes=y.shape[1])
    return X_res, y_res

def get_class_weights(y):
    """
    y: one-hot encoded array shape=(N,3)
    ‡∏Ñ‡∏∑‡∏ô dict mapping class index ‚Üí weight
    """
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô labels 0/1/2
    labels = np.argmax(y, axis=1)
    # ‡∏´‡∏≤‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
    present_classes = np.unique(labels)
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì class weight ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á
    cw = compute_class_weight(
        class_weight='balanced',
        classes=present_classes,
        y=labels
    )
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏ï‡πá‡∏° 0‚Äì2 ‡πÇ‡∏î‡∏¢‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏î ‡πÉ‡∏´‡πâ weight=1.0
    class_weights = {i: 1.0 for i in range(y.shape[1])}
    for cls, w in zip(present_classes, cw):
        class_weights[int(cls)] = float(w)
    return class_weights

def weighted_cce_loss(weight_vector):
    """
    weight_vector: 1D array ‡∏´‡∏£‡∏∑‡∏≠ list ‡∏Ç‡∏≠‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™ [w0, w1, w2]
    ‡∏Ñ‡∏∑‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô loss ‡∏ó‡∏µ‡πà scale cross‚Äêentropy ‡∏î‡πâ‡∏ß‡∏¢ weight per sample
    """
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô tensor ‡∏Ñ‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ
    weights = tf.constant(weight_vector, dtype=tf.float32)
    def loss(y_true, y_pred):
        # ‡∏õ‡∏Å‡∏ï‡∏¥ categorical_crossentropy ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô shape=(batch,)
        cce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weight ‡∏ï‡πà‡∏≠ sample ‡∏î‡πâ‡∏ß‡∏¢ dot product ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á one-hot y_true ‡∏Å‡∏±‡∏ö weights
        sample_weights = tf.reduce_sum(y_true * weights, axis=1)
        # scale loss
        return cce * sample_weights
    return loss

def sparse_focal_loss(gamma=2.0, alpha=0.25):
    """
    Focal loss ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sparse labels (y_true ‡πÄ‡∏õ‡πá‡∏ô int) 
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô float32 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á dtype mismatch
    """
    def loss_fn(y_true, y_pred):
        # 1) Cast y_pred ‚Üí float32
        y_pred = tf.cast(y_pred, tf.float32)
        # 2) Flatten y_true ‚Üí int32 vector
        y_true = tf.cast(tf.reshape(y_true, [-1]), tf.int32)

        # 3) Sparse Categorical Crossentropy (float32)
        ce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)

        # 4) ‡∏™‡∏£‡πâ‡∏≤‡∏á one-hot ‡∏Ç‡∏≠‡∏á y_true (float32)
        num_classes = tf.shape(y_pred)[-1]
        y_true_onehot = tf.one_hot(y_true, depth=num_classes, dtype=tf.float32)

        # 5) ‡∏î‡∏∂‡∏á p_t = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏£‡∏¥‡∏á
        p_t = tf.reduce_sum(y_true_onehot * y_pred, axis=1)

        # 6) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì focal weight
        weight = alpha * tf.pow(1.0 - p_t, gamma)

        # 7) ‡∏Ñ‡∏∑‡∏ô loss per-sample (float32)
        return weight * ce

    return loss_fn

class ConfusionMatrixCallback(Callback):
    def __init__(self, validation_data, labels=None, target_names=None):
        super().__init__()
        self.X_val, self.y_val = validation_data
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏Ñ‡∏•‡∏≤‡∏™‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
        self.labels       = labels       if labels       is not None else [0,1,2]
        self.target_names = target_names if target_names is not None else ['Short','Hold','Long']

    def on_epoch_end(self, epoch, logs=None):
        # 1) predict ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å label
        y_pred = np.argmax(self.model.predict(self.X_val, verbose=0), axis=-1)
        # 2) ‡∏ñ‡πâ‡∏≤ y_val ‡πÄ‡∏õ‡πá‡∏ô one-hot ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á
        if self.y_val.ndim > 1:
            y_true = np.argmax(self.y_val, axis=1)
        else:
            y_true = self.y_val
        # 3) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì CM ‡πÅ‡∏•‡∏∞ report
        cm = confusion_matrix(y_true, y_pred, labels=self.labels)
        print(f"\nEpoch {epoch+1} Confusion Matrix (labels={self.labels}):\n{cm}")
        print(classification_report(
            y_true, y_pred,
            labels=self.labels,
            target_names=self.target_names
        ))

def build_stage1_model(input_shape):
    """
    ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏¢‡∏Å Long (2) vs Not-Long (0+1)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà output ‡πÄ‡∏õ‡πá‡∏ô 2 classes, ‡πÉ‡∏ä‡πâ sparse_categorical_crossentropy
    """
    model = Sequential([
        Input(shape=input_shape),
        LSTM(64, return_sequences=False, kernel_regularizer=l2(0.01)),
        Dense(2, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

def build_stage2_model(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        LSTM(64, return_sequences=False, kernel_regularizer=l2(1e-4)),
        Dropout(0.5),
        Dense(2, activation='softmax', kernel_regularizer=l2(1e-4))
    ])
    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss=sparse_focal_loss(gamma=2.0, alpha=0.25),  # <‚Äî ‡πÉ‡∏ä‡πâ sparse version
        metrics=['accuracy']
    )
    return model

def find_threshold_by_bisect(
    data_pca: np.ndarray,
    prices: np.ndarray,
    look_back: int,
    hold_bars: int,
    desired_hold: float,
    tol: float = 0.005,
    max_iter: int = 20
) -> float:
    n = len(prices)
    max_i = n - look_back - hold_bars
    future_returns = np.empty(max_i, dtype=np.float32)
    for i in range(max_i):
        future_returns[i] = prices[i + look_back + hold_bars] / prices[i + look_back] - 1.0

    abs_ret = np.abs(future_returns)
    lo = 0.0
    hi = abs_ret.max() 

    thr = (lo + hi) / 2.0
    hold_frac = 0.0

    for _ in range(max_iter):
        thr = (lo + hi) / 2.0
        X_tmp_chk, y_tmp_chk = create_labels_from_price(
            data_pca, prices,
            look_back=look_back,
            hold_bars=hold_bars,
            thr_short=thr, thr_long=thr
        )
        labels_chk = np.argmax(y_tmp_chk, axis=1)
        hold_frac = np.mean(labels_chk == 1)

        if abs(hold_frac - desired_hold) <= tol:
            break
        # ‡∏ñ‡πâ‡∏≤ hold_frac ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ > desired_hold ‚Üí thr ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô ‚Üí ‡∏•‡∏î hi
        if hold_frac > desired_hold:
            hi = thr
        else:
            lo = thr

    return thr

def find_threshold_for_short(
    data_pca: np.ndarray,
    prices: np.ndarray,
    look_back: int,
    hold_bars: int,
    target_short_frac: float,
    low: float = 0.0001,
    high: float = 0.01,
    step: float = 0.0001
) -> tuple[float, None]:
    """
    ‡∏´‡∏≤ threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Short ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ fraction ‡∏Ç‡∏≠‡∏á Short ‚âà target_short_frac
    ‡∏Ñ‡∏∑‡∏ô (thr_short, None)
    """
    from sklearn.metrics import recall_score

    best_thr, best_diff = 0.0, float('inf')
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á labels ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÇ‡∏î‡∏¢ varying thr_short
    for thr in np.arange(low, high+step, step):
        X_tmp, y_tmp = create_labels_from_price_nonzero(
            data_pca, prices,
            look_back=look_back, hold_bars=hold_bars,
            thr_short=thr, thr_long=high  # thr_long ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö Long
        )
        labels = np.argmax(y_tmp, axis=1)
        short_frac = np.mean(labels == 0)
        diff = abs(short_frac - target_short_frac)
        if diff < best_diff:
            best_diff, best_thr = diff, thr
    return best_thr, None

def find_threshold_for_long(
    data_pca: np.ndarray,
    prices: np.ndarray,
    look_back: int,
    hold_bars: int,
    target_long_frac: float,
    low: float = 0.0001,
    high: float = 0.01,
    step: float = 0.0001
) -> tuple[None, float]:
    """
    ‡∏´‡∏≤ threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Long ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ fraction ‡∏Ç‡∏≠‡∏á Long ‚âà target_long_frac
    ‡∏Ñ‡∏∑‡∏ô (None, thr_long)
    """
    best_thr, best_diff = 0.0, float('inf')
    for thr in np.arange(low, high+step, step):
        X_tmp, y_tmp = create_labels_from_price_nonzero(
            data_pca, prices,
            look_back=look_back, hold_bars=hold_bars,
            thr_short=high, thr_long=thr  # thr_short ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö Short
        )
        labels = np.argmax(y_tmp, axis=1)
        long_frac = np.mean(labels == 2)
        diff = abs(long_frac - target_long_frac)
        if diff < best_diff:
            best_diff, best_thr = diff, thr
    return None, best_thr

class MacroF1(tf.keras.metrics.Metric):
    def __init__(self, num_classes=3, name='macro_f1', **kwargs):
        super().__init__(name=name, **kwargs)
        self.num_classes = num_classes
        # ‡πÉ‡∏™‡πà name ‡πÉ‡∏´‡πâ add_weight ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
        self.tp = self.add_weight(
            name='tp',
            shape=(num_classes,),
            initializer='zeros'
        )
        self.fp = self.add_weight(
            name='fp',
            shape=(num_classes,),
            initializer='zeros'
        )
        self.fn = self.add_weight(
            name='fn',
            shape=(num_classes,),
            initializer='zeros'
        )

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int32)
        y_true = tf.cast(y_true, tf.int32)
        for i in range(self.num_classes):
            true_i = tf.equal(y_true, i)
            pred_i = tf.equal(y_pred_labels, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(true_i, pred_i), self.dtype))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(~true_i, pred_i), self.dtype))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(true_i, ~pred_i), self.dtype))
            self.tp[i].assign_add(tp)
            self.fp[i].assign_add(fp)
            self.fn[i].assign_add(fn)

    def result(self):
        precision = tf.math.divide_no_nan(self.tp, self.tp + self.fp)
        recall    = tf.math.divide_no_nan(self.tp, self.tp + self.fn)
        f1 = 2 * precision * recall / tf.math.maximum(precision + recall, 1e-8)
        return tf.reduce_mean(f1)

    def reset_states(self):
        for v in self.variables:
            v.assign(tf.zeros_like(v))

# ====================================
# üîß Configuration (‡∏ß‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà)
# ====================================
CONFIG = {
    "symbol": "EURUSDm",
    "file_paths": {
        "M5":  r"C:\\Users\\ACE\\EURUSDm_M5_7.csv",
        "M15": r"C:\\Users\\ACE\\EURUSDm_M15_6.csv",
        "H1":  r"C:\\Users\\ACE\\EURUSDm_H1_6.csv"
    },
    "parameters": {
        "look_back":        30,
        "max_hold_period":  18,
        "min_trades":       50,
        "batch_size":       64,
        "epochs_ea22":      100,
        "epochs_ea27":      30,
        "tcn_tune_iters":   5,
        "tft_tune_iters":   5
    }
}

# ‚îÄ‚îÄ‚îÄ Test consistency snippet ‚îÄ‚îÄ‚îÄ
file_path = CONFIG["file_paths"]["M5"]
symbol    = CONFIG["symbol"]

df_raw  = load_csv(file_path)
df_prep = prepare_base_dataframe(file_path, symbol)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ index ‡πÄ‡∏õ‡πá‡∏ô DatetimeIndex ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
assert isinstance(df_raw.index, pd.DatetimeIndex)
assert isinstance(df_prep.index, pd.DatetimeIndex)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏•‡∏±‡∏Å‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏£‡∏ö
for col in ['open','high','low','close']:
    assert col in df_raw.columns and col in df_prep.columns

print(f"‚úî raw rows: {len(df_raw)} ‚Üí prepared rows: {len(df_prep)}")

# ====================================
# üéØ Section 6a: Run EA22
# ====================================
def run_ea22():
    symbol = CONFIG["symbol"]
    paths  = CONFIG["file_paths"]
    params = CONFIG["parameters"]

    # 1) Load & feature‚Äêengineering
    data = load_csv_data(paths)
    df = prepare_base_dataframe(paths["M5"], symbol, dfs_other={'M15': data['M15'], 'H1': data['H1']})
    check_dataset_sufficiency(
        df,
        look_back  = params["look_back"],
        hold_bars  = params["max_hold_period"],
        min_trades = params["min_trades"]
    )

    # 0) Detect regime ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô df
    df['regime'] = detect_market_regime(df['close'].values, n_states=3)    

    # 2) Prepare & dimensionality reduction
    features_ea22 = [c for c in df.columns if df[c].dtype != 'O']
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° 'regime' ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
    features_ea22 += ['regime']
    features_ea22 += [f'vp_bin_{i+1}' for i in range(20)]
    features_ea22 += [f'imb_{i+1}' for i in range(10)]
    features_ea22 += ['total_bid', 'total_ask', 'spread']
    features_ea22 = list(OrderedDict.fromkeys(features_ea22))
    print(f"EA22 features after dedupe: {len(features_ea22)} items")

    #  ‚Äí‚Äí‚Äí ‡∏´‡∏≤‡∏Å‡∏°‡∏µ NaN ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ffill/bfill ‡πÅ‡∏ó‡∏ô dropna() ‚Äí‚Äí‚Äí
    X_raw_df = df[features_ea22].fillna(method='ffill').fillna(method='bfill')
    X_raw = X_raw_df.values
    print("X_raw.shape:", X_raw.shape)

    # 4) Scale ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥)
    scaler = RobustScaler().fit(X_raw)
    data_pca = scaler.transform(X_raw)   # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ data_pca ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ downstream
    print("Using full feature set ‚Üí data_pca.shape =", data_pca.shape)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å feature names, scaler, pca
    os.makedirs("models/ea22", exist_ok=True)
    with open("models/ea22/feature_names_ea22.pkl", "wb") as f:
        pickle.dump(features_ea22, f)
    joblib.dump(scaler, "models/ea22/scaler22.pkl")

    prices = df['close'].values
    returns = pd.Series(prices).pct_change().dropna().values
    desired_hold = 0.20
    tol = 0.005

    max_thr = np.max(np.abs(returns))  # ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á

    # 1. ‡∏´‡∏≤ thr_short (e.g. bisect/search ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ short)
    thr_short, _ = find_threshold_for_short(
        data_pca, prices,
        look_back=params["look_back"],
        hold_bars=params["max_hold_period"],
        target_short_frac=0.20  # ‡∏ï‡∏±‡πâ‡∏á value ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    )
    # 2. ‡∏´‡∏≤ thr_long
    _, thr_long = find_threshold_for_long(
        data_pca, prices,
        look_back=params["look_back"],
        hold_bars=params["max_hold_period"],
        target_long_frac=0.20
    )

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á X_tmp, y_tmp ‡∏î‡πâ‡∏ß‡∏¢ threshold ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ
    X_tmp, y_tmp = create_labels_from_price_nonzero(
        data_pca, prices,
        look_back=params["look_back"],
        hold_bars=params["max_hold_period"],
        thr_short=thr_short,
        thr_long=thr_long
    )
    hold_frac_final = np.mean(np.argmax(y_tmp, axis=1) == 1)

    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á X_tmp, y_tmp ‡πÅ‡∏•‡πâ‡∏ß:
    n_samples, seq_len, n_feats = X_tmp.shape
    labels_tmp = np.argmax(y_tmp, axis=1)
    X_flat = X_tmp.reshape(n_samples, seq_len * n_feats)
    y_labels = np.argmax(y_tmp, axis=1)
    hold_count = np.sum(labels_tmp == 1)
    X_sel_flat, top_idx = select_features_by_mutual_info(X_flat, y_labels, k=40)

    print(f"Final Hold frac = {hold_count/len(labels_tmp):.2%} (target={desired_hold:.2%})")
    print("Before oversample (all):", np.unique(labels_tmp, return_counts=True))

    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
    # 1) ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Hold ‡πÄ‡∏•‡∏¢‡πÉ‡∏ô y_tmp ‚Üí inject dummy Hold ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á train/val
    if hold_count == 0:
        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á dummy ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÇ‡∏î‡∏¢ copy sequence ‡πÅ‡∏£‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á X_tmp
        # ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏±‡πâ‡∏á label ‡πÄ‡∏õ‡πá‡∏ô [0,1,0] (Short=0, Hold=1, Long=0)
        n_dummy = max(100, int(0.02 * len(X_tmp)))  
        # (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏ä‡∏¥‡πâ‡∏ô ‡∏´‡∏£‡∏∑‡∏≠ 1% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)  
        dummy_seq = np.repeat(X_tmp[:1], n_dummy, axis=0)
        dummy_lbl = np.tile([0,1,0], (n_dummy, 1))
        X_tmp = np.concatenate([X_tmp, dummy_seq], axis=0)
        y_tmp = np.concatenate([y_tmp, dummy_lbl], axis=0)

        labels_tmp = np.argmax(y_tmp, axis=1)
        hold_count = np.sum(labels_tmp == 1)
        print(f"Injected {n_dummy} dummy Hold ‚Üí new Hold frac = {hold_count/len(labels_tmp):.2%}")
        print("After injecting dummy, support:", np.unique(labels_tmp, return_counts=True))

    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
    # 2) ‡πÅ‡∏ö‡πà‡∏á train/val
    X_tr, X_va, y_tr, y_va = train_test_split(
        X_tmp, y_tmp,
        test_size=0.2,
        shuffle=False,
        random_state=42
    )
    y_true = np.argmax(y_va, axis=1)
    labels_tr_before = np.argmax(y_tr, axis=1)
    if np.sum(labels_tr_before == 1) == 0:
        len_orig = len(labels_tmp) - n_dummy  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô inject
        dummy_indices = list(range(len_orig, len_orig + n_dummy))
        # ‡∏¢‡πâ‡∏≤‡∏¢ 1 dummy ‡∏ó‡∏∏‡∏Å‡∏™‡∏¥‡∏ö‡∏ï‡∏±‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ X_tr ‡∏à‡∏∞‡∏°‡∏µ Hold ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ï‡∏±‡∏ß
        moved = 0
        for idx in dummy_indices:
            # ‡∏´‡∏≤ position ‡∏Ç‡∏≠‡∏á idx ‡πÉ‡∏ô X_tmp ‡∏ó‡∏µ‡πà map ‡πÑ‡∏õ X_tr / X_va
            # ‡∏≠‡∏≤‡∏®‡∏±‡∏¢ fact ‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£ train_test_split ‡πÄ‡∏°‡∏∑‡πà‡∏≠ shuffle=False ‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡πâ‡∏≤‡∏¢ 20% ‡πÄ‡∏õ‡πá‡∏ô val
            if idx < len(X_tmp) * 0.8:
                # idx ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô train (80%)
                moved += 1
                break  # ‡πÄ‡∏à‡∏≠ dummy ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô train ‡πÅ‡∏•‡πâ‡∏ß
        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ dummy ‡πÉ‡∏ô train ‡πÉ‡∏´‡πâ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å dummy_seq ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ X_tr ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        if moved == 0:
            extra_seq = dummy_seq[:1]
            extra_lbl = dummy_lbl[:1]
            X_tr = np.concatenate([X_tr, extra_seq], axis=0)
            y_tr = np.concatenate([y_tr, extra_lbl], axis=0)
            moved = 1

        print(f"Moved {moved} dummy Hold ‡πÄ‡∏Ç‡πâ‡∏≤ X_tr ‚Üí Now train support:", 
              np.unique(np.argmax(y_tr, axis=1), return_counts=True))
        
    look_back = params["look_back"]
    n_features = X_tr.shape[2]
    input_shape = (look_back, n_features)

    def build_lstm_model(hp):
        units        = hp.Int('lstm_units',   min_value=32, max_value=128, step=32)
        dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)
        lr           = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

        model = Sequential([
            Input(shape=input_shape),
            LSTM(units),
            Dropout(dropout_rate),
            Dense(3, activation='softmax')
        ])
        model.compile(
            optimizer=Adam(learning_rate=lr),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

    tuner = kt.RandomSearch(
        build_lstm_model,
        objective='val_accuracy',
        max_trials=5,
        executions_per_trial=1,
        directory='kt_logs',
        project_name='ea22_lstm'
    )

    earlystop = EarlyStopping(
        monitor='val_accuracy',    # ‡∏´‡∏£‡∏∑‡∏≠ 'val_loss' ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        patience=5,
        restore_best_weights=True,
        verbose=1
    )

    tuner.search(
        X_tr, np.argmax(y_tr, axis=1),
        epochs=20,
        validation_data=(X_va, np.argmax(y_va, axis=1)),
        callbacks=[earlystop]
    )

    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    print("Best HPS:", best_hps.values)

    def build_tuned_lstm(look_back: int, n_features: int):
        units        = best_hps.get('lstm_units')
        dropout_rate = best_hps.get('dropout_rate')
        lr           = best_hps.get('learning_rate')

        model = Sequential([
            Input(shape=(look_back, n_features)),
            LSTM(units),
            Dropout(dropout_rate),
            Dense(3, activation='softmax')
        ])
        model.compile(
            optimizer=Adam(learning_rate=lr),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
    # 4) Oversample ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô X_tr
    labels_tr_after = np.argmax(y_tr, axis=1)
    if 1 in labels_tr_after:
        X_tr, y_tr = oversample_hold_only(X_tr, y_tr, hold_class=1, target_frac=0.20)
        labels_tr = np.argmax(y_tr, axis=1)
        print("After oversample classes on train:", np.unique(labels_tr, return_counts=True))
    else:
        print("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Hold ‡πÉ‡∏ô train ‡πÅ‡∏°‡πâ‡∏´‡∏•‡∏±‡∏á inject dummy ‚Üí skip oversample_hold_only")

    tcn_params = tune_tcn(X_tr, y_tr, params["look_back"])
    tft_params = tune_tft(X_tr, y_tr, params["look_back"])

    # 10) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô build_base_fns (‡∏ô‡∏≥‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà tune ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡πÉ‡∏ä‡πâ)
    base_fns = [
        build_model_lstm_att_hp,
        build_tuned_lstm,
        build_model_cnn_lstm,
        lambda lb, nf, fp=tcn_params['filters'], kp=tcn_params['kernel']: 
            build_model_tcn(lb, nf, filters=int(fp), kernel_size=int(kp)),
        lambda lb, nf, hp=tft_params['head_size'], fd=tft_params['ff_dim']: 
            build_model_transformer(lb, nf, head_size=int(hp), ff_dim=int(fd))
    ]

    cv_scores = cross_validate_ea22(
        X_tr, y_tr,
        build_base_fns=base_fns,
        build_rf_fn=build_rf_model,
        build_meta_fn=build_meta_model,
        look_back=params["look_back"],
        batch_size=params["batch_size"],
        epochs=params["epochs_ea22"],
        n_splits=5
    )
    print(f"[EA22 CV] Mean accuracy: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}")

    # ‡∏´‡∏•‡∏±‡∏á oversample_hold_only ‡∏ö‡∏ô X_tr, y_tr
    labels_res = np.argmax(y_tr, axis=1)
    present = np.unique(labels_res)
    cw_vals = compute_class_weight(class_weight='balanced', classes=present, y=labels_res)
    class_weights_dict = {c: w for c,w in zip(present, cw_vals)}
    class_weights_dict[1] *= 1.0   # ‡∏Ç‡∏¢‡∏±‡∏ö Hold ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å 50%
    weight_vector = np.array([class_weights_dict[i] for i in [0,1,2]], dtype=np.float32)
    loss_fn = weighted_cce_loss(weight_vector)

    # 14) ‡∏™‡∏£‡πâ‡∏≤‡∏á deep_models 4 ‡πÅ‡∏ö‡∏ö ‡πÅ‡∏•‡πâ‡∏ß compile ‡∏î‡πâ‡∏ß‡∏¢ loss_fn + metrics per-class
    deep_models = []
    for build_fn in [
        build_model_lstm_att_hp,
        build_tuned_lstm,
        build_model_cnn_lstm,
        lambda lb, nf: build_model_tcn(lb, nf, filters=int(tcn_params['filters']), kernel_size=int(tcn_params['kernel'])),
        lambda lb, nf: build_model_transformer(lb, nf, head_size=int(tft_params['head_size']), ff_dim=int(tft_params['ff_dim']))
    ]:
        m = build_fn(params["look_back"], X_tr.shape[2])
        m.compile(
            optimizer=LossScaleOptimizer(Adam(learning_rate=1e-4)),
            loss=loss_fn,
            metrics=[
                'accuracy',
                MacroF1(num_classes=3),
                tf.keras.metrics.Precision(class_id=1, name='prec_hold'),
                tf.keras.metrics.Recall(class_id=1, name='rec_hold')
            ]
        )
        deep_models.append(m)

    reduce_lr = ReduceLROnPlateau(
        monitor='val_accuracy', factor=0.5,
        patience=2, min_lr=1e-6, verbose=1
    )

    # 15) ‡∏™‡∏£‡πâ‡∏≤‡∏á TensorBoard callback
    tb_cb = TensorBoard(log_dir="logs/ea22_per_class", update_freq='epoch')

    # 16) Two-Stage Training (Stage 1: Long vs Rest)
    y_lbl_tr = np.argmax(y_tr, axis=1)
    y_lbl_va = np.argmax(y_va, axis=1)
    y1_tr = (y_lbl_tr == 2).astype(int)
    y1_va = (y_lbl_va == 2).astype(int)
    
    present1 = np.unique(y1_tr)  # ‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™ [0] ‡∏´‡∏£‡∏∑‡∏≠ [1] ‡∏´‡∏£‡∏∑‡∏≠ [0,1]
    if len(present1) < 2:
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏Å‡πá‡πÉ‡∏´‡πâ default weight =1.0 ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
        class_weight_stage1 = {0:1.5, 1:1.0}
    else:
        cw1_vals = compute_class_weight(
            class_weight='balanced',
            classes=present1,
            y=y1_tr
        )
        class_weight_stage1 = {int(c): float(w) for c, w in zip(present1, cw1_vals)}
    # ‡πÄ‡∏ï‡∏¥‡∏° default weight=1.0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ (0,1)
        for c in [0, 1]:
            class_weight_stage1.setdefault(c, 1.0)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å Stage-1 Model
    m1 = build_stage1_model(input_shape=(params["look_back"], X_tr.shape[2]))
    m1.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    m1.fit(
        X_tr, y1_tr,
        validation_data=(X_va, y1_va),
        epochs=params["epochs_ea22"],
        batch_size=params["batch_size"],
        class_weight=class_weight_stage1,
        callbacks=[tb_cb, earlystop, reduce_lr, ConfusionMatrixCallback(validation_data=(X_va, y1_va))],
        verbose=1
    )

    # 2) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stage 2: Short vs Hold ‡∏ö‡∏ô ‚ÄúRest‚Äù
    #    ‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏≤ indices ‡∏ó‡∏µ‡πà Stage1 ‡∏ó‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô Rest (==0)
    p1_tr = m1.predict(X_tr).argmax(axis=1)  # predictions ‡∏ö‡∏ô train
    p1_va = m1.predict(X_va).argmax(axis=1)  # predictions ‡∏ö‡∏ô val

    # ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å Stage 2 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‚Äúlabel ‡∏à‡∏£‡∏¥‡∏á‚Äù (not predicted) ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° y_lbl_tr != 2
    mask_tr = (y_lbl_tr != 2)  # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏õ‡πá‡∏ô Short(0) ‡∏´‡∏£‡∏∑‡∏≠ Hold(1)
    mask_va = (y_lbl_va != 2)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á X_tr2, y2_tr ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ label ‡∏à‡∏£‡∏¥‡∏á y_lbl_tr
    X_tr2 = X_tr[mask_tr]
    rest_idx = np.where(y_lbl_tr != 2)[0]
    y2_tr = y_lbl_tr[mask_tr]  # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ y2_tr ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 0=Short ‡∏´‡∏£‡∏∑‡∏≠ 1=Hold

    X_va2 = X_va[mask_va]
    rest_va_idx = np.where(y_lbl_va != 2)[0]
    y2_va = y_lbl_va[mask_va]

    from imblearn.over_sampling import BorderlineSMOTE

    if np.sum(y2_tr == 1) < 100:
        sm = BorderlineSMOTE(sampling_strategy=0.3, random_state=42)
        # reshape ‚Üí 2D ‡∏Å‡πà‡∏≠‡∏ô resample
        X2_flat, y2_res = sm.fit_resample(
            X_tr2.reshape(len(X_tr2), -1),
            y2_tr
        )
        
        X_tr2 = X2_flat.reshape(-1, LOOK_BACK, X_tr.shape[2])
        y2_tr = y2_res
        print("‡πÉ‡∏ä‡πâ BorderlineSMOTE Stage‚Äâ2 ‚Üí support Short vs Hold:", np.unique(y2_tr, return_counts=True))
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î classes ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô: 0=Short, 1=Hold
    classes2 = np.array([0, 1])
    if len(np.unique(y2_tr)) < 2:
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏´‡πâ weight =1 ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
        class_weight_stage2 = {0:1.5, 1:1.0}
    else:
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì weight ‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡∏¢‡∏≤‡∏¢ Hold ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ï‡∏±‡∏ß
        cw2_vals = compute_class_weight(
            class_weight='balanced',
            classes=classes2,
            y=y2_tr
        )
        class_weight_stage2 = {
            0: float(cw2_vals[0]),
            1: float(cw2_vals[1]) * 1.3
        }

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å Stage-2 Model
    m2 = build_stage2_model(input_shape=(params["look_back"], X_tr.shape[2]))
    m2.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    m2.fit(
        X_tr2, y2_tr,
        validation_data=(X_va2, y2_va),
        epochs=params["epochs_ea22"],
        batch_size=params["batch_size"],
        class_weight=class_weight_stage2,
        callbacks=[tb_cb, earlystop, reduce_lr, ConfusionMatrixCallback(validation_data=(X_va2, y2_va), labels=[0,1], target_names=['Short','Hold'])],
        verbose=1
    )

    # 19) Train RF + Stacking Meta-Model
    rf22 = build_rf_model()
    rf22.fit(X_tr.reshape(len(X_tr), -1), y_lbl_tr)

    meta_X22 = get_base_predictions(deep_models, X_tr, rf_model=rf22)
    meta22 = build_meta_model(input_dim=meta_X22.shape[1])

    # ‡πÅ‡∏ö‡πà‡∏á train/val ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö meta-model
    X_meta_tr, X_meta_val, y_meta_tr, y_meta_val = train_test_split(
        meta_X22,
        y_lbl_tr,         # label ‡πÄ‡∏î‡∏¥‡∏° 0/1/2
        test_size=0.2,
        random_state=42,
        shuffle=False
    )

    # 6) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì class_weight ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Meta-Model
    labels_meta = np.unique(y_meta_tr)  # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô sub-train (0/1/2 ‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡∏≠‡∏≤‡∏à‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™)
    cw_meta_vals = compute_class_weight(
        class_weight='balanced',
        classes=labels_meta,
        y=y_meta_tr
    )
    class_weight_meta = {int(c): float(w) for c, w in zip(labels_meta, cw_meta_vals)}
    # ‡πÄ‡∏ï‡∏¥‡∏° default=1.0 ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 0,1,2
    for c in [0, 1, 2]:
        class_weight_meta.setdefault(c, 1.0)

    # ‚Äî‚Äî ‡∏Ç‡πâ‡∏≠ 4: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡∏≤‡∏™ ‚ÄúLong‚Äù (2) ‚Äî‚Äî 
    long_boost = 1.5  # ‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 1.2, 2.0, ‡∏Ø‡∏•‡∏Ø ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    class_weight_meta[2] = class_weight_meta[2] * long_boost
    print("Adjusted class weights:", class_weight_meta)

    # 7) Compile Meta-Model
    meta22.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # 8) ‡∏ù‡∏∂‡∏Å Meta-Model ‡∏û‡∏£‡πâ‡∏≠‡∏° class_weight ‡πÅ‡∏•‡∏∞ EarlyStopping
    meta22.fit(
        X_meta_tr, y_meta_tr,
        validation_data=(X_meta_val, y_meta_val),
        epochs=params["epochs_ea22"],
        batch_size=params["batch_size"],
        class_weight=class_weight_meta,
        callbacks=[earlystop, reduce_lr, ConfusionMatrixCallback(validation_data=(X_meta_val, y_meta_val))],
        verbose=1)

     # ---- Calibration with IsotonicRegression ----
    # (a) ‡∏î‡∏∂‡∏á P(Hold) ‡∏ö‡∏ô meta‚Äêtrain
    probs_tr = meta22.predict(X_meta_tr)    # shape=(n_tr,3)
    P_hold_tr = probs_tr[:,1]
    y_hold_tr = (y_meta_tr == 1).astype(int)
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° true label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Hold
    mask = ~np.isnan(P_hold_tr)
    
    uniq = np.unique(y_hold_tr[mask])
    if len(uniq) == 2:
        iso = IsotonicRegression(out_of_bounds='clip')
        iso.fit(P_hold_tr[mask], y_hold_tr[mask])
        def calibrate(p): 
            return iso.predict(np.nan_to_num(p, nan=0.5))
    else:
    # ‡∏ñ‡πâ‡∏≤ meta-train ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á 0 ‡πÅ‡∏•‡∏∞ 1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™ Hold ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏° calibration
        def calibrate(p): 
            return np.nan_to_num(p, nan=0.5)

    # (b) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì P(Hold) ‡∏ö‡∏ô meta‚Äêval ‡πÅ‡∏•‡πâ‡∏ß calibrate
    probs_val_meta   = meta22.predict(X_meta_val)  # ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á raw
    P_hold_cal_val = calibrate(probs_val_meta[:, 1])
    y_true_hold_val = (y_meta_val == 1).astype(int)

    # ---- 1) ‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Fixed Threshold = 0.48 ----
    hold_thr_fixed = 0.48

    # ‡∏Å‡∏±‡∏ô NaN ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Short(0)/Long(2)
    p0 = np.nan_to_num(probs_val_meta[:, 0], nan=0.0)
    p2 = np.nan_to_num(probs_val_meta[:, 2], nan=0.0)
    other_pred_val = np.where(p0 >= p2, 0, 2)

    final_pred_fixed = np.where(
        P_hold_cal_val > hold_thr_fixed,
        1,                          # Hold
        other_pred_val)

    y_meta_val_labels = y_meta_val if y_meta_val.ndim == 1 else np.argmax(y_meta_val, axis=1)
    print(f"\n=== Fixed hold_thr = {hold_thr_fixed:.2f} ===")
    print(classification_report(
        y_meta_val_labels,
        final_pred_fixed,
        target_names=['Short','Hold','Long'],
        zero_division=0
    ))
    print("Confusion Matrix:\n",  confusion_matrix(y_meta_val_labels, final_pred_fixed, labels=[0,1,2])
    )

    # ---- ‡∏´‡∏≤ optimal threshold ‡∏î‡πâ‡∏ß‡∏¢ F1‚Äêmacro ----
    best_thr, best_f1 = 0.5, 0.0
    for thr in np.linspace(0.30, 0.60, num=301):
        pred_is_hold = (P_hold_cal_val > thr).astype(int)
        if y_true_hold_val.sum() == 0:
            f1 = 0.0
        else:
            f1 = f1_score(y_true_hold_val, pred_is_hold, average='binary', pos_label=1, zero_division=0)
        if f1 > best_f1:
            best_f1, best_thr = f1, thr

    hold_thr = best_thr 
    print(f"Optimal hold_thr (binary-F1 for Hold): {best_thr:.3f}, F1 = {best_f1:.4f}")

    # ‚Äî Stage 1 & 2 evaluate with confidence threshold ‚Äî
    # 1) ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå Stage1 (Long vs Rest)
    probs1     = m1.predict(X_va)          # shape=(n_val,2)
    long_probs = probs1[:, 1]              # P(Long)
    long_thr   = 0.6                       # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ú‡∏• CV
    long_mask  = long_probs > long_thr     # Boolean array ‡∏¢‡∏≤‡∏ß n_val

    meta_X_va   = get_base_predictions(deep_models, X_va, rf_model=rf22, batch_size=params["batch_size"])
    prob_va    = meta22.predict(meta_X_va)
    P_hold_cal_va  = calibrate(prob_va[:, 1]) 

    final_pred = np.zeros(len(X_va), dtype=int)
    final_pred[long_mask] = 2
    hold_mask = (~long_mask) & (P_hold_cal_va > hold_thr)
    final_pred[hold_mask] = 1 
                                 
    # 6) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
    y_true = np.argmax(y_va, axis=1)
    print(classification_report(y_true, final_pred, target_names=['Short','Hold','Long']))
    print("Confusion Matrix:\n", confusion_matrix(y_true, final_pred, labels=[0,1,2]))

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì calibration curve
    fraction_of_pos, mean_pred_value = calibration_curve(
        y_true_hold_val,
        P_hold_cal_val,          
        n_bins=10,
        strategy='uniform'
    )

    # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü
    plt.figure(figsize=(6,6))
    plt.plot(mean_pred_value, fraction_of_pos, "s-", label="Calibrated")
    plt.plot([0,1], [0,1], "k--", label="Perfectly calibrated")
    plt.xlabel("Mean predicted probability")
    plt.ylabel("Fraction of positives")
    plt.title("Reliability Diagram ‚Äî Hold Class")
    plt.legend()
    plt.grid(True)
    plt.show()
    plt.hist(P_hold_cal, bins=10, range=(0,1), alpha=0.3, label="Prob histogram")
    plt.legend()
    plt.show()

    # ---- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏ó‡∏µ‡πà threshold ‡πÉ‡∏´‡∏°‡πà ----
    final_pred = np.where(P_hold_cal > best_thr,
                          1,
                          np.argmax(probs_val, axis=1))
    print(classification_report(y_meta_val, final_pred, target_names=['Short','Hold','Long']))
    print("Confusion Matrix:\n", confusion_matrix(y_meta_val, final_pred))

    # 20) Save artifacts
    artifacts = {
        "ea22_lstm":       deep_models[0],
        "ea22_cnn_lstm":   deep_models[1],
        "ea22_tcn":        deep_models[2],
        "ea22_transformer": deep_models[3],
        "scaler22":        scaler,
        "rf22":            rf22,
        "meta22":          meta22
    }
    save_model_bundle(artifacts, folder="models/ea22")
    logger.info("‚úÖ EA22 Single-TF Training Complete")

    # 21) Cleanup memory (‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á)
    to_delete = [
        'df','X_raw','scaled','data_pca','X_tmp','y_tmp',
        'X_tr','X_va','y_tr','y_va','deep_models',
        'rf22','meta_X22','meta22'
    ]
    for name in to_delete:
        if name in locals():
            del locals()[name]

    K.clear_session()
    gc.collect()

# ====================================
# üéØ Section 6b: Run EA27
# ====================================
def run_ea27():
    symbol = CONFIG["symbol"]
    paths  = CONFIG["file_paths"]
    params = CONFIG["parameters"]

    # 1) ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• M5, M15, H1
    data = load_csv_data(paths)
    df_m5 = prepare_base_dataframe(paths["M5"], symbol, dfs_other={'M15': data['M15'], 'H1': data['H1']})
    df_m15 = prepare_base_dataframe(paths["M15"], symbol, dfs_other={})
    df_h1 = prepare_base_dataframe(paths["H1"], symbol, dfs_other={})
    # 2) Join multi-TF; ‡πÉ‡∏ä‡πâ left join + ffill ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏•‡∏î row ‡∏´‡∏•‡∏±‡∏Å‡∏•‡∏á‡πÄ‡∏¢‡∏≠‡∏∞
    df27 = (
        df_m5
        .join(df_m15.add_suffix("_M15"), how="left")
        .fillna(method='ffill')
        .join(df_h1.add_suffix("_H1"), how="left")
        .fillna(method='ffill')
    )

    check_dataset_sufficiency(
        df27,
        look_back=params["look_back"],
        hold_bars=params["max_hold_period"],
        min_trades=params["min_trades"]
    )

    # 3) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô) + orderbook
    features_ea27 = [c for c in df27.columns if df27[c].dtype != 'O']
    features_ea27 += [f"vp_bin_{i+1}" for i in range(20)]
    features_ea27 += [f"imb_{i+1}" for i in range(10)]
    features_ea27 += ["total_bid", "total_ask", "spread"]
    features_ea27 = list(OrderedDict.fromkeys(features_ea27))

    X_raw = df27[features_ea27].dropna().values
    scaler_27 = RobustScaler().fit(X_raw)
    scaled_27 = scaler_27.transform(X_raw)
    pca_27 = PCA(n_components=0.95).fit(scaled_27)
    data_pca_27 = pca_27.transform(scaled_27)

    os.makedirs("models/ea27", exist_ok=True)
    with open("models/ea27/feature_names_ea27.pkl", "wb") as f:
        pickle.dump(features_ea27, f)
    joblib.dump(scaler_27, "models/ea27/scaler27.pkl")
    joblib.dump(pca_27, "models/ea27/pca27.pkl")

    # 4) ‡∏´‡∏≤ threshold ‡∏à‡∏≤‡∏Å price-driven label
    thr_short27, thr_long27 = find_threshold_by_grid(
        df27, data_pca_27,
        look_back=params["look_back"],
        hold_bars=params["max_hold_period"],
        low=0.0001, high=0.005, step=0.00005,
        target_hold_frac=(0.005, 0.05)
    )
    with open("models/ea27/best_threshold.pkl", "wb") as f:
        pickle.dump((thr_short27, thr_long27), f)

    # 5) ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset classification
    X_tmp, y_tmp = create_labels_from_price(
        df27,
        look_back=params["look_back"],
        hold_bars=params["max_hold_period"],
        thr_short=thr_short27,
        thr_long=thr_long27
    )
    print("Before oversample, classes:", np.unique(np.argmax(y_tmp, axis=1)))

    # 6) inject dummy hold ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
    if 1 not in np.unique(np.argmax(y_tmp, axis=1)):
        dummy_seq = np.repeat(X_tmp[:1], 20, axis=0)
        dummy_lbl = np.tile([0,1,0], (20,1))
        X_tmp = np.concatenate([X_tmp, dummy_seq], axis=0)
        y_tmp = np.concatenate([y_tmp, dummy_lbl], axis=0)

    # 7) oversample hold ~25%
    X_res, y_res = oversample_hold_only(X_tmp, y_tmp, hold_class=1, target_frac=0.25)
    print("Post-oversample classes:", np.unique(np.argmax(y_res, axis=1), return_counts=True))

    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á X_tmp, y_tmp ‡πÅ‡∏•‡πâ‡∏ß:
    labels_tmp = np.argmax(y_tmp, axis=1)
    unique, counts = np.unique(labels_tmp, return_counts=True)
    print("Before oversample:", dict(zip(unique, counts)))

    X_res, y_res = oversample_hold_only(X_tmp, y_tmp, hold_class=1, target_frac=0.12)
    labels_res = np.argmax(y_res, axis=1)
    unique2, counts2 = np.unique(labels_res, return_counts=True)
    print("After oversample:", dict(zip(unique2, counts2)))

    # 8) CV ‡∏î‡πâ‡∏ß‡∏¢ cross_validate_ea22
    base_fns27 = [
        build_model_lstm_att_hp,
        build_model_cnn_lstm,
        build_model_tcn,
        build_model_transformer
    ]
    cv27_scores = cross_validate_ea22(
        X_res, y_res,
        build_base_fns=base_fns27,
        build_rf_fn=build_rf_model,
        build_meta_fn=build_meta_model,
        look_back=params["look_back"],
        batch_size=params["batch_size"],
        epochs=params["epochs_ea27"],
        n_splits=5
    )
    print(f"[EA27 CV] Mean accuracy: {np.mean(cv27_scores):.4f} ¬± {np.std(cv27_scores):.4f}")

    # 9) train/val split ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    X_tr, X_va, y_tr, y_va = train_test_split(X_res, y_res, test_size=0.2, random_state=42, shuffle=False)

    # 10) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì class_weight ‡πÅ‡∏ö‡∏ö multi-class
    labels_res = np.argmax(y_tr, axis=1)
    classes = np.array([0,1,2])
    cw_list = compute_class_weight('balanced', classes=classes, y=labels_res)
    class_weight_multi = {int(c): float(w) for c, w in zip(classes, cw_list)}

    loss_fn = weighted_cce_loss(cw_list.astype(np.float32))

    # 11) Tune TCN/TFT (‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö EA22)
    tcn_params = tune_tcn(X_res, y_res, params["look_back"])
    tft_params = tune_tft(X_res, y_res, params["look_back"])

    # 12) ‡∏™‡∏£‡πâ‡∏≤‡∏á deep_models 4 ‡πÅ‡∏ö‡∏ö ‡πÅ‡∏•‡πâ‡∏ß compile
    deep_models = []
    for build_fn in [
        build_model_lstm_att_hp,
        build_model_cnn_lstm,
        lambda lb, nf: build_model_tcn(lb, nf, filters=int(tcn_params['filters']), kernel_size=int(tcn_params['kernel'])),
        lambda lb, nf: build_model_transformer(lb, nf, head_size=int(tft_params['head_size']), ff_dim=int(tft_params['ff_dim']))
    ]:
        m = build_fn(params["look_back"], X_tr.shape[2])
        m.compile(
            optimizer=LossScaleOptimizer(Adam(learning_rate=1e-4)),
            loss=loss_fn,
            metrics=['accuracy']
        )
        deep_models.append(m)

    tb27 = TensorBoard(log_dir="logs/ea27_per_class", update_freq='epoch')

    # 13) Two-Stage Training (Stage 1: Long vs Rest)
    lbl_tr27 = np.argmax(y_tr, axis=1)
    lbl_va27 = np.argmax(y_va, axis=1)
    y1_tr27 = (lbl_tr27 == 2).astype(int)
    y1_va27 = (lbl_va27 == 2).astype(int)
    present1_27 = np.unique(y1_tr27)
    if len(present1_27) < 2:
        class_weight_stage1_27 = {0:1.0, 1:1.0}
    else:
        cw1_27 = compute_class_weight('balanced', classes=present1_27, y=y1_tr27)
        class_weight_stage1_27 = {int(c): float(w) for c, w in zip(present1_27, cw1_27)}

    m1_27 = build_stage1_model(input_shape=(params["look_back"], X_tr.shape[2]))
    m1_27.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    m1_27.fit(
        X_tr, y1_tr27,
        validation_data=(X_va, y1_va27),
        epochs=params["epochs_ea27"],
        batch_size=params["batch_size"],
        class_weight=class_weight_stage1_27,
        callbacks=[tb27, EarlyStopping(patience=5, restore_best_weights=True)],
        verbose=1
    )

    # 14) Stage 2: Short vs Hold ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ label ‡∏à‡∏£‡∏¥‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ prediction ‡∏à‡∏≤‡∏Å Stage 1)
    mask_tr27 = (lbl_tr27 != 2)
    X_tr2_27 = X_tr[mask_tr27]
    y2_tr27 = lbl_tr27[mask_tr27]
    mask_va27 = (lbl_va27 != 2)
    X_va2_27 = X_va[mask_va27]
    y2_va27 = lbl_va27[mask_va27]
    present2_27 = np.unique(y2_tr27)
    if len(present2_27) < 2:
        class_weight_stage2_27 = {0:1.0, 1:1.0}
    else:
        cw2_27 = compute_class_weight('balanced', classes=present2_27, y=y2_tr27)
        class_weight_stage2_27 = {int(c): float(w) for c, w in zip(present2_27, cw2_27)}
        for c in [0,1]:
            class_weight_stage2_27.setdefault(c, 1.0)

    m2_27 = build_stage2_model(input_shape=(params["look_back"], X_tr.shape[2]))
    m2_27.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    m2_27.fit(
        X_tr2_27, y2_tr27,
        validation_data=(X_va2_27, y2_va27),
        epochs=params["epochs_ea27"],
        batch_size=params["batch_size"],
        class_weight=class_weight_stage2_27,
        callbacks=[tb27, EarlyStopping(patience=5, restore_best_weights=True)],
        verbose=1
    )

    # 15) Evaluate Two-Stage
    p1_va27 = m1_27.predict(X_va).argmax(axis=1)
    final_va27 = np.zeros_like(p1_va27)
    final_va27[p1_va27 == 1] = 2
    idx_nl27 = np.where(p1_va27 == 0)[0]
    final_va27[idx_nl27] = m2_27.predict(X_va[idx_nl27]).argmax(axis=1)
    print(classification_report(lbl_va27, final_va27, target_names=['Short','Hold','Long']))
    print("Confusion Matrix:\n", confusion_matrix(lbl_va27, final_va27))

    # 16) RF + XGB + Meta-model
    rf27 = build_rf_model()
    rf27.fit(X_tr.reshape(len(X_tr), -1), lbl_tr27)

    # ‡∏ï‡∏£‡∏ß‡∏à injection dummy ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™ Hold ‡πÉ‡∏ô y_tr
    if 1 not in np.unique(lbl_tr27):
        print("Injecting dummy Hold sample for XGB")
        dummy_x = X_tr[:1]
        dummy_y = np.array([1])
        X_tr = np.concatenate([X_tr, dummy_x], axis=0)
        lbl_tr27 = np.concatenate([lbl_tr27, dummy_y], axis=0)

    xgb27 = xgb.XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        use_label_encoder=False,
        eval_metric='mlogloss',
        random_state=42
    )
    xgb27.fit(X_tr.reshape(len(X_tr), -1), lbl_tr27)

    meta_X_tr27 = get_base_predictions(deep_models, X_tr, rf_model=rf27)
    print("Meta-feature shape:", meta_X_tr27.shape)

    Xm_tr27, Xm_va27, ym_tr27, ym_va27 = train_test_split(
        meta_X_tr27, lbl_tr27,
        test_size=0.2, random_state=42, shuffle=False
    )
    labels_meta27 = np.unique(ym_tr27)
    cw_meta_vals27 = compute_class_weight('balanced', classes=labels_meta27, y=ym_tr27)
    class_weight_meta27 = {int(c): float(w) for c, w in zip(labels_meta27, cw_meta_vals27)}
    for c in [0,1,2]:
        class_weight_meta27.setdefault(c, 1.0)

    meta27 = build_meta_model(input_dim=meta_X_tr27.shape[1])
    meta27.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    meta27.fit(
        Xm_tr27, ym_tr27,
        validation_data=(Xm_va27, ym_va27),
        epochs=params["epochs_ea27"],
        batch_size=params["batch_size"],
        class_weight=class_weight_meta27,
        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],
        verbose=1
    )

    # 17) Save artifacts
    artifacts = {
        "ea27_lstm":       deep_models[0],
        "ea27_cnn_lstm":   deep_models[1],
        "ea27_tcn":        deep_models[2],
        "ea27_transformer": deep_models[3],
        "scaler27":        scaler_27,
        "pca27":           pca_27,
        "rf27":            rf27,
        "xgb27":           xgb27,
        "meta27":          meta27
    }
    save_model_bundle(artifacts, folder="models/ea27")
    logger.info("‚úÖ EA27 Ensemble Training Complete")

    # 18) Cleanup
    del df_m5, df_m15, df_h1, df27, X_raw, scaled_27, data_pca_27
    del X_res, y_res, X_tr, X_va, y_tr, y_va, deep_models, rf27, xgb27, meta_X_tr27, meta27
    K.clear_session()
    gc.collect()

# Paths & params (adjust as needed)
M5_DATA_PATH   = CONFIG["file_paths"]["M5"]
M15_DATA_PATH  = CONFIG["file_paths"]["M15"]
H1_DATA_PATH   = CONFIG["file_paths"]["H1"]
LOOK_BACK      = CONFIG["parameters"]["look_back"]
SYMBOL         = CONFIG["symbol"]

def evaluate_ea22():
    print("\n--- Evaluating EA22 ---")

    # 1) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + artifacts
    with custom_object_scope({'AttentionLayer': AttentionLayer, 'TCN': TCN}):
        meta22  = load_model("models/ea22/meta22.h5", compile=False,
                             custom_objects={'AttentionLayer': AttentionLayer, 'TCN': TCN})
        m_lstm  = load_model("models/ea22/ea22_lstm.h5",       compile=False)
        m_cnn   = load_model("models/ea22/ea22_cnn_lstm.h5",   compile=False)
        m_tcn   = load_model("models/ea22/ea22_tcn.h5",        compile=False)
        m_trans = load_model("models/ea22/ea22_transformer.h5", compile=False)

    rf22     = joblib.load("models/ea22/rf22.pkl")
    scaler22 = joblib.load("models/ea22/scaler22.pkl")

    # ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡∏ù‡∏∂‡∏Å
    with open("models/ea22/feature_names_ea22.pkl", "rb") as f:
        saved_feats = pickle.load(f)

    # 2) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° DataFrame (M5) ‡∏î‡πâ‡∏ß‡∏¢ pipeline ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô train
    df = load_csv(CONFIG["file_paths"]["M5"])  # index_col='time'
    df = calculate_indicators(df)
    df = calculate_support_resistance(df)
    df = add_additional_features(df)
    df = add_confirm_entry_feature(df)
    df = enrich_df_with_tick_orderbook(df, CONFIG["symbol"])

    # 3) ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏° saved_feats
    for feat in saved_feats:
        if feat not in df.columns:
            df[feat] = 0.0

    # 4) ‡∏™‡∏£‡πâ‡∏≤‡∏á X_raw ‚Üí scale (no PCA)
    X_raw_df = df[saved_feats] \
                    .fillna(method='ffill') \
                    .fillna(method='bfill') \
                    .fillna(0.0)
    X_raw    = X_raw_df.values
    X_scaled = scaler22.transform(X_raw)

    # 5) ‡∏´‡∏≤ threshold ‡∏ö‡∏ô X_scaled
    prices      = df['close'].values
    desired_hold = 0.20
    tol          = 0.005

    thr_final = find_threshold_by_bisect(
        X_scaled,                                   # ‚Üê ‡πÉ‡∏ä‡πâ X_scaled ‡πÅ‡∏ó‡∏ô data_pca
        prices,
        look_back   = CONFIG["parameters"]["look_back"],
        hold_bars   = CONFIG["parameters"]["max_hold_period"],
        desired_hold= desired_hold,
        tol         = tol
    )
    thr_short, thr_long = thr_final, thr_final
    print(f"[Evaluate] ‡πÉ‡∏ä‡πâ thr_final = {thr_final:.6f} ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Hold‚âà{desired_hold:.0%}")

    # 6) ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏î‡πâ‡∏ß‡∏¢ create_labels_from_price
    X, y = create_labels_from_price(
        X_scaled,                                   # ‚Üê ‡πÉ‡∏ä‡πâ X_scaled
        prices,
        look_back  = CONFIG["parameters"]["look_back"],
        hold_bars  = CONFIG["parameters"]["max_hold_period"],
        thr_short  = thr_short,
        thr_long   = thr_long
    )

    # 7) ‡πÅ‡∏¢‡∏Å validation set (20% ‡∏ó‡πâ‡∏≤‡∏¢)
    n_seq  = len(X)
    n_test = int(n_seq * 0.2)
    X_train, X_val = X[:-n_test], X[-n_test:]
    y_train, y_val = y[:-n_test], y[-n_test:]

    labels_va = np.argmax(y_val, axis=1)
    print("Validation support (Short, Hold, Long):",
          np.unique(labels_va, return_counts=True))

    # 8) ‡∏ó‡∏≥ stacking & predict
    base_models = [m_lstm, m_cnn, m_tcn, m_trans]
    meta_inputs = get_base_predictions(base_models, X_val, rf_model=rf22)

    y_true = np.argmax(y_val, axis=1)
    y_pred = np.argmax(meta22.predict(meta_inputs), axis=1)

    print("EA22 Accuracy:", accuracy_score(y_true, y_pred))
    print(classification_report(
        y_true, y_pred,
        labels=[0,1,2],
        target_names=['Short','Hold','Long'],
        zero_division=0
    ))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

    return df, y_pred

def backtest_signals(prices: np.ndarray, signals: np.ndarray, hold_period: int) -> np.ndarray:
    returns = []
    for i, sig in enumerate(signals):
        if i + hold_period >= len(prices):
            break
        if sig == 2:       # long
            ret = prices[i + hold_period] / prices[i] - 1
        elif sig == 0:     # short
            ret = prices[i] / prices[i + hold_period] - 1
        else:              # hold
            ret = 0.0
        returns.append(ret)
    return np.array(returns)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Execute evaluation + backtest
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    # 1) ‡∏ù‡∏∂‡∏Å EA22 ‡∏Å‡πà‡∏≠‡∏ô
    import MetaTrader5 as mt5
    if not mt5.initialize():
        raise RuntimeError("MT5 initialization failed")
    try:
        run_ea22()
    finally:
        mt5.shutdown()

# ------------------------
# 4) View first few returns
# ------------------------
# result_df = pd.DataFrame({
#     'return': trade_returns,
#     'cum_return': equity_curve[1:]  # skip the initial zero
# })
# result_df.head()

def evaluate_ea27():
    print("\n=== EA27 Evaluation ===")

    with custom_object_scope({'AttentionLayer': AttentionLayer, 'TCN': TCN}):
        meta27 = load_model("models/ea27/meta27.h5", compile=False, custom_objects={'AttentionLayer': AttentionLayer, 'TCN': TCN})
        m_lstm27 = load_model("models/ea27/ea27_lstm.h5", compile=False)
        m_cnn27 = load_model("models/ea27/ea27_cnn_lstm.h5", compile=False)
        m_tcn27 = load_model("models/ea27/ea27_tcn.h5", compile=False)
        m_trans27 = load_model("models/ea27/ea27_transformer.h5", compile=False)

    rf27 = joblib.load("models/ea27/rf27.pkl")
    scaler27 = joblib.load("models/ea27/scaler27.pkl")
    pca27 = joblib.load("models/ea27/pca27.pkl")
    with open("models/ea27/feature_names_ea27.pkl", "rb") as f:
        features_ea27 = pickle.load(f)
    with open("models/ea27/best_threshold.pkl", "rb") as f:
        thr_short27, thr_long27 = pickle.load(f)

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° DataFrame multi-TF
    data = load_csv_data(CONFIG["file_paths"])
    df_m5 = prepare_base_dataframe(CONFIG["file_paths"]["M5"], CONFIG["symbol"], dfs_other={'M15': data['M15'], 'H1': data['H1']})
    df_m15 = prepare_base_dataframe(CONFIG["file_paths"]["M15"], CONFIG["symbol"], dfs_other={})
    df_h1 = prepare_base_dataframe(CONFIG["file_paths"]["H1"], CONFIG["symbol"], dfs_other={})
    df27 = (
        df_m5
        .join(df_m15.add_suffix("_M15"), how="left").fillna(method='ffill')
        .join(df_h1.add_suffix("_H1"), how="left").fillna(method='ffill')
    )

    # Feature transform
    X_raw27 = df27[features_ea27].dropna().values
    X_scaled27 = scaler27.transform(X_raw27)
    X_pca27 = pca27.transform(X_scaled27)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset hold-out
    X27, y27 = create_labels_from_price(
        df27,
        look_back=CONFIG["parameters"]["look_back"],
        hold_bars=CONFIG["parameters"]["max_hold_period"],
        thr_short=thr_short27,
        thr_long=thr_long27
    )
    _, X_val27, _, y_val27 = train_test_split(X27, y27, test_size=0.2, random_state=42, shuffle=False)
    print("Eval classes:", np.unique(np.argmax(y_val27, axis=1)))

    # stacking + predict
    base_models27 = [m_lstm27, m_cnn27, m_tcn27, m_trans27]
    meta_inputs27 = get_base_predictions(base_models27, X_val27, rf_model=rf27)

    y_true27 = np.argmax(y_val27, axis=1)
    y_pred27 = np.argmax(meta27.predict(meta_inputs27), axis=1)

    print("EA27 Accuracy:", accuracy_score(y_true27, y_pred27))
    print(classification_report(y_true27, y_pred27, labels=[0,1,2], target_names=['Short','Hold','Long'], zero_division=0))
    print("Confusion Matrix:\n", confusion_matrix(y_true27, y_pred27))
    return df27, y_pred27
    
# ====================================
# üéØ Section 6c: Updated main()
# ====================================
def main():
    if not mt5.initialize():
        raise RuntimeError("MT5 initialization failed")
    try:
        run_ea22()
    finally:
        mt5.shutdown()

if __name__ == "__main__":
    main()
